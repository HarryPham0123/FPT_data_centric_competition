{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Mosaic_augmentation.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyOHs6wnKnPkL47Xde/4H40V"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JE8dZHIg4Iog","executionInfo":{"status":"ok","timestamp":1640598604353,"user_tz":-420,"elapsed":4479,"user":{"displayName":"Hoang Pham Viet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQXuRknMb-ZNCRChVKvYhPD-B8MW4JxHigPUlsoA=s64","userId":"13405977886381099576"}},"outputId":"9fc4dd39-1af1-4b02-aa6e-6c559926f510"},"source":["from google.colab import drive\n","\n","drive.mount('/content/gdrive', force_remount=True)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","metadata":{"id":"8motu0a0PRjH"},"source":["import os\n","\n","# (WARNING!!!) DIRECT THE PATH TO \"Data-Competition\" folder\n","path = '.../Data-Competition'\n","os.chdir(path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d3Z-lZjc4xAy","executionInfo":{"status":"ok","timestamp":1640598607358,"user_tz":-420,"elapsed":3010,"user":{"displayName":"Hoang Pham Viet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQXuRknMb-ZNCRChVKvYhPD-B8MW4JxHigPUlsoA=s64","userId":"13405977886381099576"}},"outputId":"9966726d-fc64-4773-9219-cf5f5eac33d9"},"source":["!pip install -U albumentations"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: albumentations in /usr/local/lib/python3.7/dist-packages (1.1.0)\n","Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (0.18.3)\n","Requirement already satisfied: qudida>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from albumentations) (0.0.4)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations) (3.13)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.4.1)\n","Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.19.5)\n","Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (4.5.4.60)\n","Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from qudida>=0.0.4->albumentations) (1.0.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from qudida>=0.0.4->albumentations) (3.10.0.2)\n","Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2.4.1)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (1.2.0)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2.6.3)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2021.11.2)\n","Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (3.2.2)\n","Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (7.1.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (0.11.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (3.0.6)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (2.8.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.3.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.15.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.0.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.1.0)\n"]}]},{"cell_type":"markdown","metadata":{"id":"k2kYtJM95XGR"},"source":["# Library"]},{"cell_type":"code","metadata":{"id":"6Nw9Jm2x5nhV"},"source":["import pandas as pd\n","import numpy as np\n","import glob\n","import cv2\n","import os\n","import re\n","\n","from PIL import Image\n","\n","import albumentations as A\n","from albumentations.pytorch.transforms import ToTensorV2, ToTensor\n","\n","import torch\n","import torchvision\n","\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection import FasterRCNN\n","from torchvision.models.detection.rpn import AnchorGenerator\n","\n","from torch.utils.data import DataLoader, Dataset\n","from torch.utils.data.sampler import SequentialSampler\n","\n","from matplotlib import pyplot as plt\n","\n","# DIR_TRAIN_VAL = './dataset_origin/images/train_val'\n","DIR_TRAIN = './dataset_origin/images/train'\n","DIR_VAL = './dataset_origin/images/val'\n","DIR_TEST = './dataset_origin/images/public_test'\n","\n","\n","SEED = 42\n","def seed_everything(seed=SEED):\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    # random.seed(seed)\n","    # tf.random.set_seed(seed)\n","\n","seed_everything(SEED)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ecndPV_A5wQM"},"source":["# Load train_csv data"]},{"cell_type":"code","metadata":{"id":"phhrwgtX5xph"},"source":["train_df = pd.read_csv('./dataset_origin/train_csv.csv', index_col=False)\n","val_df = pd.read_csv('./dataset_origin/val_csv.csv', index_col=False) \n","test_df = pd.read_csv('./dataset_origin/test_csv.csv', index_col=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8sb5xs24EpBe","executionInfo":{"status":"ok","timestamp":1640598624548,"user_tz":-420,"elapsed":9,"user":{"displayName":"Hoang Pham Viet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQXuRknMb-ZNCRChVKvYhPD-B8MW4JxHigPUlsoA=s64","userId":"13405977886381099576"}},"outputId":"5f14ed28-a591-466e-c8bc-4dec61b2d7fc"},"source":["print(f'There are totally {len(train_df[\"image_id\"].unique())} images in total dataset')\n","print(f'There are totally {len(val_df[\"image_id\"].unique())} images in val dataset')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["There are totally 937 images in total dataset\n","There are totally 152 images in val dataset\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"oyxPpLLIQL35","executionInfo":{"status":"ok","timestamp":1640598624549,"user_tz":-420,"elapsed":8,"user":{"displayName":"Hoang Pham Viet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQXuRknMb-ZNCRChVKvYhPD-B8MW4JxHigPUlsoA=s64","userId":"13405977886381099576"}},"outputId":"1e997064-f24a-4069-939d-b9ffaf9127d9"},"source":["train_df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-4bd1d49a-30f6-4f91-832c-461c64d8f7d8\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>image_id</th>\n","      <th>width</th>\n","      <th>height</th>\n","      <th>label</th>\n","      <th>x</th>\n","      <th>y</th>\n","      <th>w</th>\n","      <th>h</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1035</td>\n","      <td>960.0</td>\n","      <td>960.0</td>\n","      <td>1.0</td>\n","      <td>0.704688</td>\n","      <td>0.522917</td>\n","      <td>0.034375</td>\n","      <td>0.098611</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1035</td>\n","      <td>960.0</td>\n","      <td>960.0</td>\n","      <td>1.0</td>\n","      <td>0.551562</td>\n","      <td>0.201389</td>\n","      <td>0.045312</td>\n","      <td>0.077778</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>289</td>\n","      <td>960.0</td>\n","      <td>960.0</td>\n","      <td>1.0</td>\n","      <td>0.241858</td>\n","      <td>0.605691</td>\n","      <td>0.059107</td>\n","      <td>0.162602</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>504</td>\n","      <td>960.0</td>\n","      <td>960.0</td>\n","      <td>1.0</td>\n","      <td>0.296875</td>\n","      <td>0.501389</td>\n","      <td>0.064062</td>\n","      <td>0.077778</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>504</td>\n","      <td>960.0</td>\n","      <td>960.0</td>\n","      <td>2.0</td>\n","      <td>0.791016</td>\n","      <td>0.022917</td>\n","      <td>0.021094</td>\n","      <td>0.031944</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1565</th>\n","      <td>1047</td>\n","      <td>960.0</td>\n","      <td>960.0</td>\n","      <td>1.0</td>\n","      <td>0.454688</td>\n","      <td>0.227083</td>\n","      <td>0.040625</td>\n","      <td>0.073611</td>\n","    </tr>\n","    <tr>\n","      <th>1566</th>\n","      <td>1047</td>\n","      <td>960.0</td>\n","      <td>960.0</td>\n","      <td>0.0</td>\n","      <td>0.734375</td>\n","      <td>0.472222</td>\n","      <td>0.053125</td>\n","      <td>0.077778</td>\n","    </tr>\n","    <tr>\n","      <th>1567</th>\n","      <td>1047</td>\n","      <td>960.0</td>\n","      <td>960.0</td>\n","      <td>1.0</td>\n","      <td>0.402734</td>\n","      <td>0.059028</td>\n","      <td>0.032031</td>\n","      <td>0.054167</td>\n","    </tr>\n","    <tr>\n","      <th>1568</th>\n","      <td>1047</td>\n","      <td>960.0</td>\n","      <td>960.0</td>\n","      <td>1.0</td>\n","      <td>0.848437</td>\n","      <td>0.021528</td>\n","      <td>0.020313</td>\n","      <td>0.031944</td>\n","    </tr>\n","    <tr>\n","      <th>1569</th>\n","      <td>1047</td>\n","      <td>960.0</td>\n","      <td>960.0</td>\n","      <td>0.0</td>\n","      <td>0.684375</td>\n","      <td>0.022222</td>\n","      <td>0.020313</td>\n","      <td>0.038889</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1570 rows × 8 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4bd1d49a-30f6-4f91-832c-461c64d8f7d8')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-4bd1d49a-30f6-4f91-832c-461c64d8f7d8 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-4bd1d49a-30f6-4f91-832c-461c64d8f7d8');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["      image_id  width  height  label         x         y         w         h\n","0         1035  960.0   960.0    1.0  0.704688  0.522917  0.034375  0.098611\n","1         1035  960.0   960.0    1.0  0.551562  0.201389  0.045312  0.077778\n","2          289  960.0   960.0    1.0  0.241858  0.605691  0.059107  0.162602\n","3          504  960.0   960.0    1.0  0.296875  0.501389  0.064062  0.077778\n","4          504  960.0   960.0    2.0  0.791016  0.022917  0.021094  0.031944\n","...        ...    ...     ...    ...       ...       ...       ...       ...\n","1565      1047  960.0   960.0    1.0  0.454688  0.227083  0.040625  0.073611\n","1566      1047  960.0   960.0    0.0  0.734375  0.472222  0.053125  0.077778\n","1567      1047  960.0   960.0    1.0  0.402734  0.059028  0.032031  0.054167\n","1568      1047  960.0   960.0    1.0  0.848437  0.021528  0.020313  0.031944\n","1569      1047  960.0   960.0    0.0  0.684375  0.022222  0.020313  0.038889\n","\n","[1570 rows x 8 columns]"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"tn38kJh-AdiI"},"source":["## Mosaic"]},{"cell_type":"code","metadata":{"id":"i2rzPJa7AdAB"},"source":["from sklearn.utils import shuffle\n","import random\n","\n","class FPTDatasetMosaic(Dataset):\n","  def __init__(self, dataframe, image_dir, transforms=None):\n","    super().__init__()\n","\n","    self.df = dataframe  # Annotation & Image's ID dataframe\n","    self.transforms = transforms  # Albumentation's augmentation\n","    self.image_ids = shuffle(dataframe['image_id'].unique())  # Image's ID\n","    self.labels = [np.zeros((0, 4), dtype=np.float32)] * len(self.image_ids) # Image's bboxes\n","    self.class_labels = [np.zeros((0, 1), dtype=np.float32)] * len(self.image_ids)  # Image's label\n","    self.img_size = 960\n","    self.image_dir = image_dir\n","    self.mosaic = True\n","    im_w = 1280\n","    im_h = 720\n","\n","    # Loop through each image (Each image might containt multiple bboxes & labels)\n","    for i, img_id in enumerate(self.image_ids):\n","      records = self.df[self.df['image_id'] == img_id]\n","      labels = records[['label', 'x', 'y', 'w', 'h']].values  # Annotations\n","      self.labels[i] = np.array(labels)\n","\n","      \n","  def __getitem__(self, index: int):\n","    if self.mosaic == True:\n","      # Load mosaic\n","      img, labels = load_mosaic(self, index)\n","      shapes = None\n","      img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # BGR to RGB      \n","      return img, labels\n","\n","  def __len__(self) -> int:\n","    return self.image_ids.shape[0]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ntuiUAqOINAg"},"source":["def random_affine(img, targets=(), degrees=10, translate=.1, scale=.1, shear=10, border=0):\n","    # torchvision.transforms.RandomAffine(degrees=(-10, 10), translate=(.1, .1), scale=(.9, 1.1), shear=(-10, 10))\n","    # https://medium.com/uruvideo/dataset-augmentation-with-random-homographies-a8f4b44830d4\n","\n","    if targets is None:  # targets = [cls, xyxy]\n","        targets = []\n","    height = img.shape[0] + border * 2\n","    width = img.shape[1] + border * 2\n","\n","    # Rotation and Scale\n","    R = np.eye(3)\n","    a = random.uniform(-degrees, degrees)\n","    # a += random.choice([-180, -90, 0, 90])  # add 90deg rotations to small rotations\n","    s = random.uniform(1 - scale, 1 + scale)\n","    R[:2] = cv2.getRotationMatrix2D(angle=a, center=(img.shape[1] / 2, img.shape[0] / 2), scale=s)\n","\n","    # Translation\n","    T = np.eye(3)\n","    T[0, 2] = random.uniform(-translate, translate) * img.shape[0] + border  # x translation (pixels)\n","    T[1, 2] = random.uniform(-translate, translate) * img.shape[1] + border  # y translation (pixels)\n","\n","    # Shear\n","    S = np.eye(3)\n","    S[0, 1] = math.tan(random.uniform(-shear, shear) * math.pi / 180)  # x shear (deg)\n","    S[1, 0] = math.tan(random.uniform(-shear, shear) * math.pi / 180)  # y shear (deg)\n","\n","    # Combined rotation matrix\n","    M = S @ T @ R  # ORDER IS IMPORTANT HERE!!\n","    if (border != 0) or (M != np.eye(3)).any():  # image changed\n","        img = cv2.warpAffine(img, M[:2], dsize=(width, height), flags=cv2.INTER_LINEAR, borderValue=(114, 114, 114))\n","\n","    # Transform label coordinates (Important!!!)\n","    n = len(targets)\n","    if n:\n","        # warp points\n","        xy = np.ones((n * 4, 3))\n","        xy[:, :2] = targets[:, [1, 2, 3, 4, 1, 4, 3, 2]].reshape(n * 4, 2)  # x1y1, x2y2, x1y2, x2y1\n","        xy = (xy @ M.T)[:, :2].reshape(n, 8)\n","\n","        # create new boxes\n","        x = xy[:, [0, 2, 4, 6]]\n","        y = xy[:, [1, 3, 5, 7]]\n","        xy = np.concatenate((x.min(1), y.min(1), x.max(1), y.max(1))).reshape(4, n).T\n","\n","        # # apply angle-based reduction of bounding boxes\n","        # radians = a * math.pi / 180\n","        # reduction = max(abs(math.sin(radians)), abs(math.cos(radians))) ** 0.5\n","        # x = (xy[:, 2] + xy[:, 0]) / 2\n","        # y = (xy[:, 3] + xy[:, 1]) / 2\n","        # w = (xy[:, 2] - xy[:, 0]) * reduction\n","        # h = (xy[:, 3] - xy[:, 1]) * reduction\n","        # xy = np.concatenate((x - w / 2, y - h / 2, x + w / 2, y + h / 2)).reshape(4, n).T\n","\n","        # Explanation of this part of code: https://github.com/ultralytics/yolov5/issues/448\n","        # reject warped points outside of image\n","        xy[:, [0, 2]] = xy[:, [0, 2]].clip(0, width)\n","        xy[:, [1, 3]] = xy[:, [1, 3]].clip(0, height)\n","        w = xy[:, 2] - xy[:, 0]\n","        h = xy[:, 3] - xy[:, 1]\n","        area = w * h\n","        area0 = (targets[:, 3] - targets[:, 1]) * (targets[:, 4] - targets[:, 2])\n","        ar = np.maximum(w / (h + 1e-16), h / (w + 1e-16))  # aspect ratio\n","        i = (w > 4) & (h > 4) & (area / (area0 * s + 1e-16) > 0.2) & (ar < 10)\n","\n","        targets = targets[i]\n","        targets[:, 1:5] = xy[i]\n","\n","    return img, targets"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F7am6SoPmbSg"},"source":["def load_mosaic(self, index):\n","    \"\"\"Load image in a mosaic form _ combines 4 training images into one in certain ratios (instead of only two in CutMix\"\"\"\n","\n","    labels4 = []\n","    s = self.img_size  # Size of image\n","    xc, yc = [int(random.uniform(s * 0.5, s * 1.5)) for _ in range(2)]  # mosaic center x, y\n","    indices = [index] + [random.randint(0, len(self.labels) - 1) for _ in range(3)]  # 3 additional image indices\n","    for i, index in enumerate(indices):\n","        # Load image\n","        img, (h, w) = load_image(self, index)\n","\n","        # place img in img4\n","        if i == 0:  # top left\n","            img4 = np.full(shape=(s * 2, s * 2, img.shape[2]), fill_value=114, dtype=np.uint8)  # base image with 4 tiles\n","            x1a, y1a, x2a, y2a = max(xc - w, 0), max(yc - h, 0), xc, yc  # xmin, ymin, xmax, ymax (large image)\n","            x1b, y1b, x2b, y2b = w - (x2a - x1a), h - (y2a - y1a), w, h  # xmin, ymin, xmax, ymax (small image)\n","        elif i == 1:  # top right\n","            x1a, y1a, x2a, y2a = xc, max(yc - h, 0), min(xc + w, s * 2), yc\n","            x1b, y1b, x2b, y2b = 0, h - (y2a - y1a), min(w, x2a - x1a), h\n","        elif i == 2:  # bottom left\n","            x1a, y1a, x2a, y2a = max(xc - w, 0), yc, xc, min(s * 2, yc + h)\n","            x1b, y1b, x2b, y2b = w - (x2a - x1a), 0, max(xc, w), min(y2a - y1a, h)\n","        elif i == 3:  # bottom right\n","            x1a, y1a, x2a, y2a = xc, yc, min(xc + w, s * 2), min(s * 2, yc + h)\n","            x1b, y1b, x2b, y2b = 0, 0, min(w, x2a - x1a), min(y2a - y1a, h)\n","\n","        img4[y1a:y2a, x1a:x2a] = img[y1b:y2b, x1b:x2b]  # img4[ymin:ymax, xmin:xmax]\n","        padw = x1a - x1b\n","        padh = y1a - y1b\n","\n","        # Labels\n","        x = self.labels[index]\n","        labels = x.copy()\n","        if x.size > 0:  # Normalized xywh to pixel xyxy format\n","            labels[:, 1] = w * (x[:, 1] - x[:, 3] / 2) + padw\n","            labels[:, 2] = h * (x[:, 2] - x[:, 4] / 2) + padh\n","            labels[:, 3] = w * (x[:, 1] + x[:, 3] / 2) + padw\n","            labels[:, 4] = h * (x[:, 2] + x[:, 4] / 2) + padh\n","        labels4.append(labels)\n","\n","    # Concat/clip labels\n","    if len(labels4):\n","        labels4 = np.concatenate(labels4, 0)\n","        # np.clip(labels4[:, 1:] - s / 2, 0, s, out=labels4[:, 1:])  # use with center crop\n","        np.clip(labels4[:, 1:], 0, 2 * s, out=labels4[:, 1:])  # use with random_affine\n","\n","    # Reason should add \"random_affine()\" in mosaic https://github.com/ultralytics/yolov5/issues/448\n","    img4, labels4 = random_affine(img4, labels4,\n","                                  degrees=1.98 * 2,\n","                                  translate=0.05 * 2,\n","                                  scale=0.05 * 2,\n","                                  shear=0.641 * 2,\n","                                  border=-s // 2)  # border to remove\n","\n","    return img4, labels4\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FttL3A0Wddm0"},"source":["def arr_to_str(arr, count_f):\n","  \"\"\"Transform arr to string & save them to folder dataset_aug\"\"\"\n","  result = str()\n","\n","  # Arr to String\n","  label_1 = int(arr[0])\n","  result = str(label_1) + ' '\n","  line = ' '.join([str(item) for item in arr[1:]]) + '\\n'\n","  result = result + line\n","  return result\n","\n","def pascal_to_yolo(xmin, ymin, xmax, ymax, image_width=640, image_height=640):\n","  x_coord = (xmin + xmax) / 2 / image_width\n","  y_coord = (ymin + ymax) / 2 / image_height\n","  shape_width = (xmax - xmin) / image_width\n","  shape_height = (ymax - ymin) / image_height\n","  return x_coord, y_coord, shape_width, shape_height\n","\n","def collate_fn(batch):\n","    return tuple(zip(*batch))\n","\n","import random, math\n","import tensorflow as tf\n","\n","def load_image(self, index):\n","  # loads 1 image from dataset, returns img, original hw, resized hw\n","  \"\"\"Load 1 image from dataset\n","  Input:\n","    index: idx to search for image's id\n","  Output:\n","    img, hw_original, hw_resized \"\"\"\n","  # Read an image using opencv2\n","  image_id = self.image_ids[index]\n","  img = cv2.imread(f'{self.image_dir}/{image_id}.jpg', cv2.IMREAD_COLOR)\n","    \n","  assert img is not None, 'Image Not Found ' + imgpath\n","  h0, w0 = img.shape[:2]  # orig hw\n","  return img, (h0, w0)  # img, hw_original"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def mosaic_filter(num_img, train_df, DIR_TRAIN):\n","  \"\"\"\n","    Create & filter only mosaic image with # of labels in each image larger than 1\n","    Input:\n","      num_img: (int) # of mosaic images wanted to create\n","      train_df: (df) .csv metadata file of train dataset wanted to do augmentation\n","      DIR_TRAIN: (str) path direct to train's folder\n","    Output:\n","      image_lst: list of mosaic images\n","      target_lst: list of appropriate mosaic labels\n","  \"\"\"\n","  a = 0\n","  image_lst = list()\n","  target_lst = list()\n","\n","  train_dataset = FPTDatasetMosaic(train_df, DIR_TRAIN)  # 792 images\n","  train_data_loader = DataLoader(\n","      train_dataset,\n","      batch_size=15,\n","      shuffle=True,\n","      num_workers=4,\n","      collate_fn=collate_fn\n","  )\n","\n","  while (a < num_img):\n","    images, targets = next(iter(train_data_loader))\n","    for image, target in zip(images, targets):\n","      if len(target) > 1:\n","        image_lst.append(image)\n","        target_lst.append(target)\n","        a += 1\n","      else:\n","        continue\n","\n","      if a == num_img:\n","        break\n","\n","  return image_lst, target_lst\n","\n","\n","# Create 50 mosaic images based \"train\" dataset folder\n","images, targets = mosaic_filter(50, train_df, DIR_TRAIN)"],"metadata":{"id":"paz7VYLcRtPs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def save_mosaic(images, targets, save_labels_path, save_images_path):\n","  \"\"\"Save the mosaic images & labels into image's folder & label's folder\n","  Input:\n","    images: (list) of mosaic images (np) \n","    targets: (list) of associated mosaic labels (np)\n","    save_labels_path: (str) path folder used to save .txt labels\n","    save_images_path: (str) path folder used to save .jpg images\n","  \"\"\"\n","  a1 = 0\n","  b1 = 0\n","\n","  for img, label in zip(images, targets):\n","    height, width = img.shape[:2]\n","    a1 += 1\n","\n","    txt_file = open(f'{save_labels_path}/img_mosaic_{a1}.txt', 'w')\n","    # Through each bbox of an image\n","    for j in range(len(label)):\n","      # Normalize the box's annotation after augmentation (AS requirement from competition)\n","      a,b,c,d = pascal_to_yolo(label[j][1], label[j][2], label[j][3], label[j][4], width, height)\n","      label_yolo = np.array([label[j][0], a, b, c, d])\n","      label_yolo = arr_to_str(label_yolo, a1)\n","\n","      # Save the string for txt file\n","      txt_file.write(label_yolo)\n","    txt_file.close()\n","  print(f'FINISH SAVING MOSAIC LABELS TO FOLDER: {save_labels_path}')\n","\n","  # Save images into folder \"images/train\"\n","  for img in images:\n","    b1 += 1\n","    im = Image.fromarray(img, \"RGB\")\n","    im.save(f'{save_images_path}/img_mosaic_{b1}.jpg')\n","  print(f'FINISH SAVING MOSAIC IMAGES TO FOLDER: {save_images_path}')"],"metadata":{"id":"x7GQ7f9_R39v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save the created mosaic images to folder\n","save_mosaic(images, targets, \"./dataset_aug/labels/train\", \"./dataset_aug/images/train\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tZz_BAs4R3d4","executionInfo":{"status":"ok","timestamp":1640599011069,"user_tz":-420,"elapsed":84948,"user":{"displayName":"Hoang Pham Viet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQXuRknMb-ZNCRChVKvYhPD-B8MW4JxHigPUlsoA=s64","userId":"13405977886381099576"}},"outputId":"b718d405-16c4-4868-c08a-59b7529fc8b3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["FINISH SAVING MOSAIC LABELS TO FOLDER: ./dataset_aug/labels/train\n","FINISH SAVING MOSAIC IMAGES TO FOLDER: ./dataset_aug/images/train\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"_HZMbykyUvln"},"execution_count":null,"outputs":[]}]}