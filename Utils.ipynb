{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Utils.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMbfe2uJ22SZ9+f/sk28P7R"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Libraries"],"metadata":{"id":"6GsgOXMv2GQq"}},{"cell_type":"code","source":["import glob\n","import pandas as pd\n","import numpy as np\n","from PIL import Image\n","import os"],"metadata":{"id":"Kfnq6u_70_qI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Utils functions"],"metadata":{"id":"zAYYY8--2JCx"}},{"cell_type":"markdown","source":["### Generate metadata .csv file"],"metadata":{"id":"-2wHGqAX4p7Y"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"S0jHEdCr0uN0"},"outputs":[],"source":["def read_txt_folder(folder_path):\n","  \"\"\"Read all txt annotation files & return a dataframe containing them\n","  Input:\n","    folder_path : folder's path contained txt files\n","  Output:\n","    txt_csv: (df) csv file for the labels of those dataset\n","  \"\"\"\n","\n","  txt_csv = list()\n","  # make sure there's a slash to the folder path \n","  folder_path += \"\" if folder_path[-1] == \"/\" else \"/\"\n","  # get all text files\n","  txt_files = glob.glob(folder_path + \"*.txt\")\n","\n","  # Read each txt file\n","  for txt_file in txt_files:\n","    id = [txt_file.strip().split('/')[-1][:-4], 960.0, 960.0]\n","    # Read the content of file\n","    with open(txt_file, 'rt') as fd:\n","      lines = fd.readlines()\n","      for line in lines:\n","        box = line.strip().split(' ')\n","        txt_csv.append(id+box)\n","  \n","  return txt_csv\n","\n","\n","# train_csv = read_txt_folder('./dataset/labels/train')\n","# anno_train = pd.DataFrame(train_csv, columns=['image_id', 'width', 'height', 'label', 'x', 'y', 'w', 'h'])\n","# anno_train[['width', 'height', 'label', 'x', 'y', 'w', 'h']].astype(float)\n","\n","# anno_train.to_csv('./dataset/train_csv.csv', index=False)"]},{"cell_type":"markdown","source":["### Delete files (.jpg or .txt) from folder based on list of file's names"],"metadata":{"id":"TThFY9vH4o35"}},{"cell_type":"code","source":["# DELETE all the added images & annotation files from train_val folder\n","def delete_files_list(folder_path, indx_lst):\n","  \"\"\"Delete all img_{}.jpeg or img_{}.txt files from a given folder given the list of indexes (file's names)\n","  Inputs:\n","    folder_path: (str) path direct to folder\n","    indx_lst: (list) of deleted file's names\n","  \"\"\"\n","  a = 0\n","  # (OPTION 1) DELETE IMAGE\n","  # make sure there's a slash to the folder path \n","  folder_path += \"\" if folder_path[-1] == \"/\" else \"/\"\n","  # get all text files\n","  img_files = glob.glob(folder_path + \"*.jpg\")\n","\n","  for img_f in img_files:\n","    # Extract the name (id) of txt annotation file\n","    img_f = img_f.strip().split('/')[-1][:-4]\n","\n","    if int(img_f) in indx_lst:\n","\n","      # Remove file from folder\n","      os.remove(f\"{folder_path}{img_f}.jpg\")\n","      # print(f\"{folder_path}{img_f}.jpg\")\n","\n","\n","  # # (OPTION 2) DELETE TXT FILE\n","  # # make sure there's a slash to the folder path \n","  # folder_path += \"\" if folder_path[-1] == \"/\" else \"/\"\n","  # # get all text files\n","  # txt_files = glob.glob(folder_path + \"*.txt\")\n","\n","  # for txt_f in txt_files:\n","  #   # Extract the name (id) of txt annotation file\n","  #   txt_f = txt_f.strip().split('/')[-1][:-4]\n","\n","  #   if int(txt_f) in indx_lst:\n","  #     a += 1\n","  #     # Remove file from folder\n","  #     os.remove(f\"{folder_path}{txt_f}.txt\")\n","  #     # print(f\"{folder_path}{txt_f}.txt\")\n","\n","\n","# delete_files_list('./dataset/images/train', train_lst)\n","# delete_files_list('./dataset/labels/train', train_lst)"],"metadata":{"id":"DegzrrTX2MqF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Delete all files (.jpg or .txt) from a folder based on the name's pattern"],"metadata":{"id":"QPNZifyY8PUC"}},{"cell_type":"code","source":["def delete_files_base_pattern(folder_path, pattern):\n","  \"\"\"Delete all img_{}.jpeg or img_{}.txt files from a given folder base on the name's pattern\n","  Inputs:\n","    folder_path: (str) path direct to folder\n","    pattern: (str) pattern of deleted file's names\n","  \"\"\"\n","  a = 0\n","  # (OPTION 1) DELETE IMAGE\n","  # make sure there's a slash to the folder path \n","  folder_path += \"\" if folder_path[-1] == \"/\" else \"/\"\n","  # get all text files\n","  img_files = glob.glob(folder_path + \"*.jpg\")\n","\n","  for img_f in img_files:\n","    # Extract the name (id) of txt annotation file\n","    img_f = img_f.strip().split('/')[-1][:-4]\n","    # Filter only image with name \"img_{}.jpeg\"\n","    result = re.findall(pattern, img_f)\n","    if len(result):\n","      # Remove file from folder\n","      os.remove(folder_path+result[0]+'.jpg')\n","      # print(folder_path+result[0]+'.jpg')\n","\n","\n","  # # (OPTION 2) DELETE TXT FILE\n","  # # make sure there's a slash to the folder path \n","  # folder_path += \"\" if folder_path[-1] == \"/\" else \"/\"\n","  # # get all text files\n","  # txt_files = glob.glob(folder_path + \"*.txt\")\n","\n","  # for txt_f in txt_files:\n","  #   # Extract the name (id) of txt annotation file\n","  #   txt_f = txt_f.strip().split('/')[-1][:-4]\n","  #   # Filter only image with pattern's name\n","  #   result = re.findall(pattern, txt_f)\n","  #   if len(result):\n","  #     a += 1\n","  #     # Remove file from folder\n","  #     os.remove(folder_path+result[0]+'.txt')\n","  #     # print(folder_path+result[0]+'.txt')\n","\n","\n","# delete_files_base_pattern(\"./dataset_aug/labels/train/\", r\"img_mosaic_.*\")\n","# delete_files_base_pattern(\"./dataset_aug/images/train/\", r\"img_mosaic_.*\")"],"metadata":{"id":"e-VFo_uo8O8y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Bounding boxes types conversion\n","Types of bounding boxes: https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/"],"metadata":{"id":"AqSw49SF2MtA"}},{"cell_type":"code","source":["def pascal_to_yolo(xmin, ymin, xmax, ymax, image_width=640, image_height=640):\n","  \"\"\"Convert between pascal to yolo bboxes type\"\"\"\n","  x_coord = (xmin + xmax) / 2 / image_width\n","  y_coord = (ymin + ymax) / 2 / image_height\n","  shape_width = (xmax - xmin) / image_width\n","  shape_height = (ymax - ymin) / image_height\n","  return x_coord, y_coord, shape_width, shape_height\n","\n","\n","def yolo_to_pascal(x, y, w, h, width, height):\n","  \"\"\"Convert between yolo to pascal bboxes type\"\"\"\n","  xmax = int((x*width) + (w * width)/2.0)\n","  xmin = int((x*width) - (w * width)/2.0)\n","  ymax = int((y*height) + (h * height)/2.0)\n","  ymin = int((y*height) - (h * height)/2.0)\n","  return xmin, ymin, xmax, ymax\n","\n","def yolo_to_coco(bbox, orig_w, orig_h):\n","  \"\"\"Convert between yolo to coco bboxes type\"\"\"\n","  bbox[:, 2] = bbox[:, 2]*orig_w\n","  bbox[:, 3] = bbox[:, 3]*orig_h\n","  bbox[:, 1] = bbox[:, 1]*orig_h - (bbox[:, 3]/2)\n","  bbox[:, 0] = bbox[:, 0]*orig_w - (bbox[:, 2]/2)\n","  return bbox"],"metadata":{"id":"RBWuv5QN2MwT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Count number of .jpg & .txt files in the folder"],"metadata":{"id":"hzWZ621f2M0f"}},{"cell_type":"code","source":["def count_files_img(folder_path):\n","  \"\"\"Count all .jpg files from the folder\n","  Input:\n","    folder_path: (str) path to folder contained .jpg files\n","  Outputs:\n","    jpg_count: (int) total # of .jpf files\"\"\"\n","  # make sure there's a slash to the folder path \n","  folder_path += \"\" if folder_path[-1] == \"/\" else \"/\"\n","  # get all text files\n","  img_files = glob.glob(folder_path + \"*.jpg\")\n","  jpg_count = len(img_files)\n","  return jpg_count\n","\n","def count_files_txt(folder_path):\n","  \"\"\"Count all .txt files from the folder\n","  Input:\n","    folder_path: (str) path to folder contained .txt files\n","  Outputs:\n","    txt_count: (int) total # of .txt files\"\"\"\n","  # make sure there's a slash to the folder path \n","  folder_path += \"\" if folder_path[-1] == \"/\" else \"/\"\n","  # get all text files\n","  img_files = glob.glob(folder_path + \"*.txt\")\n","  txt_count = len(img_files)\n","  return txt_count\n","\n","\n","# print(f\"Number of train's images (jpg) in the folder: {count_files_img('./dataset/images/train')}\")\n","# print(f\"Number of train's labels (txt) in the folder: {count_files_txt('./dataset/labels/train')}\")"],"metadata":{"id":"go7cs30e2M5Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Load & Copy .jpg images or .txt files from a folder to another folder"],"metadata":{"id":"o0WNHKUY2M9L"}},{"cell_type":"code","source":["# DONE\n","def load_copy_images(start_folder, end_folder):\n","  \"\"\"Load all the .jpg images from the start_folder & save them into end_folder\n","  Inputs:\n","    start_folder: (str) path direct to the start's folder\n","    end_folder: (str) paht direct to the end's folder\n","  \"\"\"\n","\n","  # make sure there's a slash to the folder path \n","  start_folder += \"\" if start_folder[-1] == \"/\" else \"/\"\n","  # get all text files\n","  img_files = glob.glob(start_folder + \"*.jpg\")\n","\n","  for img in img_files:\n","    # Load the images from start_folder (dataset)\n","    img_arr = cv2.imread(img, cv2.IMREAD_COLOR)\n","    assert img_arr is not None, 'Image Not Found ' + imgpath\n","    img_arr = cv2.cvtColor(img_arr, cv2.COLOR_BGR2RGB)  # BGR to RGB\n","\n","\n","    # Extract the name (id) of images\n","    img_id = img.strip().split('/')[-1][:-4]\n","\n","    # Save images to end_folder (dataset_aug)\n","    im = Image.fromarray(img_arr, \"RGB\")\n","    im.save(f\"{end_folder}/{img_id}.jpg\")\n","\n","# load_copy_images('./dataset/images/train', './dataset_aug/images/train')"],"metadata":{"id":"qr1v4ID82NAw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# DONE\n","def load_copy_anno_txt(start_folder, end_folder):\n","  \"\"\"Load & copy annotation .txt files from start_folder to end_folder\n","  Inputs:\n","    start_folder: (str) path direct to the start's folder\n","    end_folder: (str) paht direct to the end's folder\n","  \"\"\"\n","  # make sure there's a slash to the folder path \n","  start_folder += \"\" if start_folder[-1] == \"/\" else \"/\"\n","\n","  # get all text files\n","  txt_files = glob.glob(start_folder + \"*.txt\")\n","  for txt_f in txt_files:\n","    # Extract the name (id) of txt annotation file\n","    txt_id = txt_f.strip().split('/')[-1][:-4]\n","\n","    with open(txt_f, 'r') as fd:\n","      lines = fd.readlines()\n","      # Save the string to txt file in end_folder (dataset_aug)\n","      txt_file = open(f'{end_folder}/{txt_id}.txt', 'w')\n","      for line in lines:\n","        txt_file.write(line)  # Write each txt line into a new file\n","      txt_file.close()\n","\n","# public_test txt folder\n","# load_copy_anno_txt('./dataset/labels/train', './dataset_aug/labels/train')"],"metadata":{"id":"SDNrkz7L2NEa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Load images & txt labels file based on the name's pattern"],"metadata":{"id":"LTzLJ7euZ1zO"}},{"cell_type":"code","source":["# DONE\n","def pattern_copy_images(start_folder, end_folder, pattern):\n","  \"\"\"Load all the .jpeg images from the start_folder & save them into end_folder\"\"\"\n","\n","  # make sure there's a slash to the folder path \n","  start_folder += \"\" if start_folder[-1] == \"/\" else \"/\"\n","  # get all text files\n","  img_files = glob.glob(start_folder + \"*.jpg\")\n","\n","  for img in img_files:\n","    # Load the images from start_folder (dataset)\n","    img_arr = cv2.imread(img, cv2.IMREAD_COLOR)\n","    assert img_arr is not None, 'Image Not Found ' + imgpath\n","    img_arr = cv2.cvtColor(img_arr, cv2.COLOR_BGR2RGB)  # BGR to RGB\n","\n","    # Extract the name (id) of images\n","    img_id = img.strip().split('/')[-1][:-4]\n","    # Filter only image with pattern's name\n","    result = re.findall(pattern, img_id)\n","\n","    if len(result):\n","      # Save images to end_folder (dataset_aug)\n","      im = Image.fromarray(img_arr, \"RGB\")\n","      print(im, f\"{end_folder}/{result[0]}.jpg\")\n","      im.save(f\"{end_folder}/{result[0]}.jpg\")\n"],"metadata":{"id":"54CCg02OZ1tv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Convert all file's images from .jpeg to .jpg in a folder"],"metadata":{"id":"7bpO5yBs2NIL"}},{"cell_type":"code","source":["def jpeg_to_jpg(start_path, end_path):\n","  \"\"\"Convert all image type .jpeg from a start_path (folder) to type .jpg & save them to end_path (folder)\"\"\"\n","  # make sure there's a slash to the folder path \n","  start_path += \"\" if start_path[-1] == \"/\" else \"/\"\n","  # get all text files\n","  img_files = glob.glob(start_path + \"*.jpeg\")\n","\n","  for img_f in img_files:\n","    # importing the image \n","    im = Image.open(img_f)\n","    # Extract the name (id) of txt annotation file\n","    img_id = img_f.strip().split('/')[-1][:-5]\n","    # converting to jpg\n","    rgb_im = im.convert(\"RGB\")\n","    # exporting the image\n","    rgb_im.save(end_path+img_id+'.jpg')\n","\n","\n","# jpeg_to_jpg('./dataset/images/train_jpeg', './dataset/images/train_jpg')\n","# jpeg_to_jpg('./dataset/images/val_jpeg', './dataset/images/val_jpg/')"],"metadata":{"id":"CveBfTz42NL6"},"execution_count":null,"outputs":[]}]}