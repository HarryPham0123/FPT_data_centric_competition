{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Experiment_augmentation.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyPJ5g9cgpi6BYo/b+9s0VV/"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JE8dZHIg4Iog","executionInfo":{"status":"ok","timestamp":1640178466984,"user_tz":-420,"elapsed":3713,"user":{"displayName":"Hoang Pham Viet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjqSUz9UAYIe9qEo0tiprwACQv8lgsyFb8T2wzpcg=s64","userId":"13405977886381099576"}},"outputId":"3d933a7e-4510-45d2-a382-3d8bcd2aba52"},"source":["from google.colab import drive\n","\n","drive.mount('/content/gdrive', force_remount=True)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","metadata":{"id":"8motu0a0PRjH"},"source":["import os\n","\n","path = '/content/gdrive/My Drive/Data-Centric_Competition/Data-Competition'\n","os.chdir(path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d3Z-lZjc4xAy","executionInfo":{"status":"ok","timestamp":1640177307890,"user_tz":-420,"elapsed":2745,"user":{"displayName":"Hoang Pham Viet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjqSUz9UAYIe9qEo0tiprwACQv8lgsyFb8T2wzpcg=s64","userId":"13405977886381099576"}},"outputId":"bb7f33e5-7724-483f-beb8-a7ecbf3e7540"},"source":["!pip install -U albumentations"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: albumentations in /usr/local/lib/python3.7/dist-packages (1.1.0)\n","Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.19.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.4.1)\n","Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (0.18.3)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations) (6.0)\n","Requirement already satisfied: qudida>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from albumentations) (0.0.4)\n","Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (4.5.4.60)\n","Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from qudida>=0.0.4->albumentations) (1.0.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from qudida>=0.0.4->albumentations) (3.10.0.2)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2021.11.2)\n","Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (3.2.2)\n","Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2.4.1)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2.6.3)\n","Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (8.4.0)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (1.2.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (3.0.6)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (0.11.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.15.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.0.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.1.0)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kxP02t3T40zp","executionInfo":{"status":"ok","timestamp":1640177310834,"user_tz":-420,"elapsed":2948,"user":{"displayName":"Hoang Pham Viet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjqSUz9UAYIe9qEo0tiprwACQv8lgsyFb8T2wzpcg=s64","userId":"13405977886381099576"}},"outputId":"6a898694-4558-421e-eaa4-4a5e5edff61a"},"source":["!pip install -r ./requirements.txt"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.7/dist-packages (from -r ./requirements.txt (line 4)) (3.2.2)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from -r ./requirements.txt (line 5)) (1.19.5)\n","Requirement already satisfied: opencv-python>=4.1.2 in /usr/local/lib/python3.7/dist-packages (from -r ./requirements.txt (line 6)) (4.1.2.30)\n","Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.7/dist-packages (from -r ./requirements.txt (line 7)) (8.4.0)\n","Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.7/dist-packages (from -r ./requirements.txt (line 8)) (6.0)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from -r ./requirements.txt (line 9)) (1.4.1)\n","Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from -r ./requirements.txt (line 10)) (1.10.0+cu111)\n","Requirement already satisfied: torchvision>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from -r ./requirements.txt (line 11)) (0.11.1+cu111)\n","Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from -r ./requirements.txt (line 12)) (4.62.3)\n","Requirement already satisfied: tensorboard>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from -r ./requirements.txt (line 15)) (2.7.0)\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from -r ./requirements.txt (line 19)) (0.11.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r ./requirements.txt (line 20)) (1.1.5)\n","Requirement already satisfied: thop in /usr/local/lib/python3.7/dist-packages (from -r ./requirements.txt (line 34)) (0.0.31.post2005241907)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r ./requirements.txt (line 4)) (2.8.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r ./requirements.txt (line 4)) (1.3.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r ./requirements.txt (line 4)) (3.0.6)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r ./requirements.txt (line 4)) (0.11.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7.0->-r ./requirements.txt (line 10)) (3.10.0.2)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r ./requirements.txt (line 15)) (0.6.1)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r ./requirements.txt (line 15)) (0.12.0)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r ./requirements.txt (line 15)) (1.42.0)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r ./requirements.txt (line 15)) (3.17.3)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r ./requirements.txt (line 15)) (1.35.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r ./requirements.txt (line 15)) (1.0.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r ./requirements.txt (line 15)) (3.3.6)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r ./requirements.txt (line 15)) (0.37.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r ./requirements.txt (line 15)) (0.4.6)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r ./requirements.txt (line 15)) (57.4.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r ./requirements.txt (line 15)) (1.8.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r ./requirements.txt (line 15)) (2.23.0)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->-r ./requirements.txt (line 20)) (2018.9)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.4.1->-r ./requirements.txt (line 15)) (1.15.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r ./requirements.txt (line 15)) (4.8)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r ./requirements.txt (line 15)) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r ./requirements.txt (line 15)) (4.2.4)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r ./requirements.txt (line 15)) (1.3.0)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.4.1->-r ./requirements.txt (line 15)) (4.8.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.4.1->-r ./requirements.txt (line 15)) (3.6.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r ./requirements.txt (line 15)) (0.4.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.4.1->-r ./requirements.txt (line 15)) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.4.1->-r ./requirements.txt (line 15)) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.4.1->-r ./requirements.txt (line 15)) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.4.1->-r ./requirements.txt (line 15)) (3.0.4)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r ./requirements.txt (line 15)) (3.1.1)\n"]}]},{"cell_type":"markdown","metadata":{"id":"k2kYtJM95XGR"},"source":["# Library"]},{"cell_type":"code","metadata":{"id":"6Nw9Jm2x5nhV"},"source":["import pandas as pd\n","import numpy as np\n","import glob\n","import cv2\n","import os\n","import re\n","\n","from PIL import Image\n","\n","import albumentations as A\n","from albumentations.pytorch.transforms import ToTensorV2, ToTensor\n","\n","import torch\n","import torchvision\n","\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection import FasterRCNN\n","from torchvision.models.detection.rpn import AnchorGenerator\n","\n","from torch.utils.data import DataLoader, Dataset\n","from torch.utils.data.sampler import SequentialSampler\n","\n","from matplotlib import pyplot as plt\n","import random\n","\n","DIR_TRAIN = './dataset_origin/images/train'\n","DIR_VAL = './dataset_origin/images/val'\n","DIR_TEST = './dataset_origin/images/public_test'\n","DIR_KAGGLE = \"./dataset_origin/data_kaggle/images/\"\n","\n","\n","SEED = 42\n","def seed_everything(seed=SEED):\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    # tf.random.set_seed(seed)\n","\n","seed_everything(SEED)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Generate meta .csv files"],"metadata":{"id":"obN1Z0CYRIFc"}},{"cell_type":"code","source":["import glob\n","import pandas as pd\n","import numpy as np\n","\n","def merge_per_folder(folder_path):\n","  \"\"\"Read all txt annotation files & return a dataframe containing them\n","  Input:\n","    folder_path : folder's path contained txt files\n","  Output:\n","    Name of the output file the merged lines will be written to.\n","  \"\"\"\n","\n","  train_csv = list()\n","  # make sure there's a slash to the folder path \n","  folder_path += \"\" if folder_path[-1] == \"/\" else \"/\"\n","  # get all text files\n","  txt_files = glob.glob(folder_path + \"*.txt\")\n","\n","  # Read each txt file\n","  for txt_file in txt_files:\n","    id = [txt_file.strip().split('/')[-1][:-4], 960.0, 960.0]\n","    # Read the content of file\n","    with open(txt_file, 'rt') as fd:\n","      lines = fd.readlines()\n","      for line in lines:\n","        box = line.strip().split(' ')\n","        train_csv.append(id+box)\n","  \n","  return train_csv\n","\n","# train_csv = merge_per_folder('./dataset_origin/labels/train')\n","# val_csv = merge_per_folder('./dataset_origin/labels/val')\n","# test_csv = merge_per_folder('./dataset_origin/labels/public_test')"],"metadata":{"id":"C3oldVHVRH3s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# df of Yolov5's bounding annotation\n","anno_train_csv = pd.DataFrame(train_csv, columns=['image_id', 'width', 'height', 'label', 'x', 'y', 'w', 'h'])\n","anno_train_csv[['width', 'height', 'label', 'x', 'y', 'w', 'h']] = anno_train_csv[['width', 'height', 'label', 'x', 'y', 'w', 'h']].astype(float)\n","\n","# anno_train_csv.to_csv('./dataset_origin/train_csv.csv', index=False)"],"metadata":{"id":"koj1yBqyRHek"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ecndPV_A5wQM"},"source":["# Load train_csv data"]},{"cell_type":"code","metadata":{"id":"phhrwgtX5xph"},"source":["train_df = pd.read_csv('./dataset_origin/train_csv.csv', index_col=False)\n","val_df = pd.read_csv('./dataset_origin/val_csv.csv', index_col=False) \n","test_df = pd.read_csv('./dataset_origin/test_csv.csv', index_col=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8sb5xs24EpBe","executionInfo":{"status":"ok","timestamp":1640177319350,"user_tz":-420,"elapsed":10,"user":{"displayName":"Hoang Pham Viet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjqSUz9UAYIe9qEo0tiprwACQv8lgsyFb8T2wzpcg=s64","userId":"13405977886381099576"}},"outputId":"a12650b9-90e8-47aa-f8a7-b1a9442fc65a"},"source":["print(f'There are totally {len(train_df[\"image_id\"].unique())} images in total dataset')\n","print(f'There are totally {len(val_df[\"image_id\"].unique())} images in val dataset')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["There are totally 937 images in total dataset\n","There are totally 152 images in val dataset\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"oyxPpLLIQL35","executionInfo":{"status":"ok","timestamp":1640177319351,"user_tz":-420,"elapsed":10,"user":{"displayName":"Hoang Pham Viet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjqSUz9UAYIe9qEo0tiprwACQv8lgsyFb8T2wzpcg=s64","userId":"13405977886381099576"}},"outputId":"7434d1b9-3126-48dc-995f-8221c2d3abe4"},"source":["train_df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-9d9400b8-cbbf-4735-80d8-c6b7261c40a4\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>image_id</th>\n","      <th>width</th>\n","      <th>height</th>\n","      <th>label</th>\n","      <th>x</th>\n","      <th>y</th>\n","      <th>w</th>\n","      <th>h</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1035</td>\n","      <td>960.0</td>\n","      <td>960.0</td>\n","      <td>1.0</td>\n","      <td>0.704688</td>\n","      <td>0.522917</td>\n","      <td>0.034375</td>\n","      <td>0.098611</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1035</td>\n","      <td>960.0</td>\n","      <td>960.0</td>\n","      <td>1.0</td>\n","      <td>0.551562</td>\n","      <td>0.201389</td>\n","      <td>0.045312</td>\n","      <td>0.077778</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>289</td>\n","      <td>960.0</td>\n","      <td>960.0</td>\n","      <td>1.0</td>\n","      <td>0.241858</td>\n","      <td>0.605691</td>\n","      <td>0.059107</td>\n","      <td>0.162602</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>504</td>\n","      <td>960.0</td>\n","      <td>960.0</td>\n","      <td>1.0</td>\n","      <td>0.296875</td>\n","      <td>0.501389</td>\n","      <td>0.064062</td>\n","      <td>0.077778</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>504</td>\n","      <td>960.0</td>\n","      <td>960.0</td>\n","      <td>2.0</td>\n","      <td>0.791016</td>\n","      <td>0.022917</td>\n","      <td>0.021094</td>\n","      <td>0.031944</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1565</th>\n","      <td>1047</td>\n","      <td>960.0</td>\n","      <td>960.0</td>\n","      <td>1.0</td>\n","      <td>0.454688</td>\n","      <td>0.227083</td>\n","      <td>0.040625</td>\n","      <td>0.073611</td>\n","    </tr>\n","    <tr>\n","      <th>1566</th>\n","      <td>1047</td>\n","      <td>960.0</td>\n","      <td>960.0</td>\n","      <td>0.0</td>\n","      <td>0.734375</td>\n","      <td>0.472222</td>\n","      <td>0.053125</td>\n","      <td>0.077778</td>\n","    </tr>\n","    <tr>\n","      <th>1567</th>\n","      <td>1047</td>\n","      <td>960.0</td>\n","      <td>960.0</td>\n","      <td>1.0</td>\n","      <td>0.402734</td>\n","      <td>0.059028</td>\n","      <td>0.032031</td>\n","      <td>0.054167</td>\n","    </tr>\n","    <tr>\n","      <th>1568</th>\n","      <td>1047</td>\n","      <td>960.0</td>\n","      <td>960.0</td>\n","      <td>1.0</td>\n","      <td>0.848437</td>\n","      <td>0.021528</td>\n","      <td>0.020313</td>\n","      <td>0.031944</td>\n","    </tr>\n","    <tr>\n","      <th>1569</th>\n","      <td>1047</td>\n","      <td>960.0</td>\n","      <td>960.0</td>\n","      <td>0.0</td>\n","      <td>0.684375</td>\n","      <td>0.022222</td>\n","      <td>0.020313</td>\n","      <td>0.038889</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1570 rows × 8 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9d9400b8-cbbf-4735-80d8-c6b7261c40a4')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-9d9400b8-cbbf-4735-80d8-c6b7261c40a4 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-9d9400b8-cbbf-4735-80d8-c6b7261c40a4');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["      image_id  width  height  label         x         y         w         h\n","0         1035  960.0   960.0    1.0  0.704688  0.522917  0.034375  0.098611\n","1         1035  960.0   960.0    1.0  0.551562  0.201389  0.045312  0.077778\n","2          289  960.0   960.0    1.0  0.241858  0.605691  0.059107  0.162602\n","3          504  960.0   960.0    1.0  0.296875  0.501389  0.064062  0.077778\n","4          504  960.0   960.0    2.0  0.791016  0.022917  0.021094  0.031944\n","...        ...    ...     ...    ...       ...       ...       ...       ...\n","1565      1047  960.0   960.0    1.0  0.454688  0.227083  0.040625  0.073611\n","1566      1047  960.0   960.0    0.0  0.734375  0.472222  0.053125  0.077778\n","1567      1047  960.0   960.0    1.0  0.402734  0.059028  0.032031  0.054167\n","1568      1047  960.0   960.0    1.0  0.848437  0.021528  0.020313  0.031944\n","1569      1047  960.0   960.0    0.0  0.684375  0.022222  0.020313  0.038889\n","\n","[1570 rows x 8 columns]"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["## Filter images having people behide doors & in the dark"],"metadata":{"id":"wI3mqWopQbD9"}},{"cell_type":"code","source":["# Some images ids needed to be augmented more to increase the accuracy\n","id_lst = [102, 20, 13, 27, 57, 66, 160, 192, 218, 217,219,408,674,801,632,923,222,428,560,545,874,881,229,509,531,575,584,576,870,871,885,\n","          157,158,396,397,393,885,854,214,848,421,622,918,919,118,330,4,111,119,511,520,586,333,336,161,394,541,544,547,878,611,613]\n","len(id_lst)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yBOaId4hQa8_","executionInfo":{"status":"ok","timestamp":1640177319351,"user_tz":-420,"elapsed":9,"user":{"displayName":"Hoang Pham Viet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjqSUz9UAYIe9qEo0tiprwACQv8lgsyFb8T2wzpcg=s64","userId":"13405977886381099576"}},"outputId":"54376bee-ada2-4d68-bae2-97477d1cc68c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["62"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["behide_door_df = train_df.loc[[True if id in id_lst else False for id in train_df['image_id']]]\n","behide_door_df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"JZclGem0QaPH","executionInfo":{"status":"ok","timestamp":1640177320035,"user_tz":-420,"elapsed":6,"user":{"displayName":"Hoang Pham Viet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjqSUz9UAYIe9qEo0tiprwACQv8lgsyFb8T2wzpcg=s64","userId":"13405977886381099576"}},"outputId":"e574ac5f-1bb4-4d20-eac8-6fbfe5a8b41b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-c89e2722-3525-49b4-b072-eba114d70134\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>image_id</th>\n","      <th>width</th>\n","      <th>height</th>\n","      <th>label</th>\n","      <th>x</th>\n","      <th>y</th>\n","      <th>w</th>\n","      <th>h</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>68</th>\n","      <td>854</td>\n","      <td>960.0</td>\n","      <td>960.0</td>\n","      <td>1.0</td>\n","      <td>0.519141</td>\n","      <td>0.511806</td>\n","      <td>0.042969</td>\n","      <td>0.112500</td>\n","    </tr>\n","    <tr>\n","      <th>110</th>\n","      <td>881</td>\n","      <td>960.0</td>\n","      <td>960.0</td>\n","      <td>1.0</td>\n","      <td>0.327734</td>\n","      <td>0.184722</td>\n","      <td>0.039844</td>\n","      <td>0.088889</td>\n","    </tr>\n","    <tr>\n","      <th>136</th>\n","      <td>102</td>\n","      <td>960.0</td>\n","      <td>960.0</td>\n","      <td>1.0</td>\n","      <td>0.630078</td>\n","      <td>0.349306</td>\n","      <td>0.063281</td>\n","      <td>0.118056</td>\n","    </tr>\n","    <tr>\n","      <th>147</th>\n","      <td>923</td>\n","      <td>960.0</td>\n","      <td>960.0</td>\n","      <td>1.0</td>\n","      <td>0.406250</td>\n","      <td>0.135417</td>\n","      <td>0.035937</td>\n","      <td>0.073611</td>\n","    </tr>\n","    <tr>\n","      <th>148</th>\n","      <td>923</td>\n","      <td>960.0</td>\n","      <td>960.0</td>\n","      <td>0.0</td>\n","      <td>0.779297</td>\n","      <td>0.115278</td>\n","      <td>0.022656</td>\n","      <td>0.036111</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1542</th>\n","      <td>870</td>\n","      <td>960.0</td>\n","      <td>960.0</td>\n","      <td>1.0</td>\n","      <td>0.481250</td>\n","      <td>0.362500</td>\n","      <td>0.060937</td>\n","      <td>0.119444</td>\n","    </tr>\n","    <tr>\n","      <th>1543</th>\n","      <td>870</td>\n","      <td>960.0</td>\n","      <td>960.0</td>\n","      <td>1.0</td>\n","      <td>0.343359</td>\n","      <td>0.168056</td>\n","      <td>0.032031</td>\n","      <td>0.066667</td>\n","    </tr>\n","    <tr>\n","      <th>1558</th>\n","      <td>509</td>\n","      <td>960.0</td>\n","      <td>960.0</td>\n","      <td>1.0</td>\n","      <td>0.755078</td>\n","      <td>0.331944</td>\n","      <td>0.046094</td>\n","      <td>0.105556</td>\n","    </tr>\n","    <tr>\n","      <th>1559</th>\n","      <td>509</td>\n","      <td>960.0</td>\n","      <td>960.0</td>\n","      <td>1.0</td>\n","      <td>0.870703</td>\n","      <td>0.431250</td>\n","      <td>0.025781</td>\n","      <td>0.087500</td>\n","    </tr>\n","    <tr>\n","      <th>1560</th>\n","      <td>509</td>\n","      <td>960.0</td>\n","      <td>960.0</td>\n","      <td>1.0</td>\n","      <td>0.633984</td>\n","      <td>0.204167</td>\n","      <td>0.019531</td>\n","      <td>0.058333</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>134 rows × 8 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c89e2722-3525-49b4-b072-eba114d70134')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-c89e2722-3525-49b4-b072-eba114d70134 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-c89e2722-3525-49b4-b072-eba114d70134');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["      image_id  width  height  label         x         y         w         h\n","68         854  960.0   960.0    1.0  0.519141  0.511806  0.042969  0.112500\n","110        881  960.0   960.0    1.0  0.327734  0.184722  0.039844  0.088889\n","136        102  960.0   960.0    1.0  0.630078  0.349306  0.063281  0.118056\n","147        923  960.0   960.0    1.0  0.406250  0.135417  0.035937  0.073611\n","148        923  960.0   960.0    0.0  0.779297  0.115278  0.022656  0.036111\n","...        ...    ...     ...    ...       ...       ...       ...       ...\n","1542       870  960.0   960.0    1.0  0.481250  0.362500  0.060937  0.119444\n","1543       870  960.0   960.0    1.0  0.343359  0.168056  0.032031  0.066667\n","1558       509  960.0   960.0    1.0  0.755078  0.331944  0.046094  0.105556\n","1559       509  960.0   960.0    1.0  0.870703  0.431250  0.025781  0.087500\n","1560       509  960.0   960.0    1.0  0.633984  0.204167  0.019531  0.058333\n","\n","[134 rows x 8 columns]"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["## Filter images having different type of masks"],"metadata":{"id":"kiaIhGWOCpRE"}},{"cell_type":"code","source":["id_lst = [12, 17, 225, 256, 39, 609, 659, 760, 797, 98, 160, 86, 634, 638, 639, 651, 650, 222, 223, 350, 661, 663, 678, 943, 518, 509, 531, \n","          870, 871, 396, 313, 422, 626, 1037, 234, 323, 1038, 497, 586, 873, 621, 631, 644, 13, 662, 660, 1050, 1035]\n","\n","len(id_lst)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dLrzAFwsCpJS","executionInfo":{"status":"ok","timestamp":1640177320036,"user_tz":-420,"elapsed":6,"user":{"displayName":"Hoang Pham Viet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjqSUz9UAYIe9qEo0tiprwACQv8lgsyFb8T2wzpcg=s64","userId":"13405977886381099576"}},"outputId":"865b5d03-d084-491f-f8b7-d3bee34be07c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["48"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["diff_mask_df = train_df.loc[[True if id in id_lst else False for id in train_df['image_id']]]\n","diff_mask_df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"OiGHpSWfCo7X","executionInfo":{"status":"ok","timestamp":1640177320036,"user_tz":-420,"elapsed":5,"user":{"displayName":"Hoang Pham Viet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjqSUz9UAYIe9qEo0tiprwACQv8lgsyFb8T2wzpcg=s64","userId":"13405977886381099576"}},"outputId":"ccce2fc8-5c99-44bc-ef06-0a234a3578e2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-2a081a3d-36a5-4e28-996a-63b9224d7b0f\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>image_id</th>\n","      <th>width</th>\n","      <th>height</th>\n","      <th>label</th>\n","      <th>x</th>\n","      <th>y</th>\n","      <th>w</th>\n","      <th>h</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1035</td>\n","      <td>960.0</td>\n","      <td>960.0</td>\n","      <td>1.0</td>\n","      <td>0.704688</td>\n","      <td>0.522917</td>\n","      <td>0.034375</td>\n","      <td>0.098611</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1035</td>\n","      <td>960.0</td>\n","      <td>960.0</td>\n","      <td>1.0</td>\n","      <td>0.551562</td>\n","      <td>0.201389</td>\n","      <td>0.045312</td>\n","      <td>0.077778</td>\n","    </tr>\n","    <tr>\n","      <th>111</th>\n","      <td>659</td>\n","      <td>960.0</td>\n","      <td>960.0</td>\n","      <td>1.0</td>\n","      <td>0.629297</td>\n","      <td>0.297917</td>\n","      <td>0.049219</td>\n","      <td>0.134722</td>\n","    </tr>\n","    <tr>\n","      <th>112</th>\n","      <td>659</td>\n","      <td>960.0</td>\n","      <td>960.0</td>\n","      <td>1.0</td>\n","      <td>0.791797</td>\n","      <td>0.624306</td>\n","      <td>0.061719</td>\n","      <td>0.109722</td>\n","    </tr>\n","    <tr>\n","      <th>141</th>\n","      <td>17</td>\n","      <td>960.0</td>\n","      <td>960.0</td>\n","      <td>1.0</td>\n","      <td>0.424609</td>\n","      <td>0.602083</td>\n","      <td>0.069531</td>\n","      <td>0.115278</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1559</th>\n","      <td>509</td>\n","      <td>960.0</td>\n","      <td>960.0</td>\n","      <td>1.0</td>\n","      <td>0.870703</td>\n","      <td>0.431250</td>\n","      <td>0.025781</td>\n","      <td>0.087500</td>\n","    </tr>\n","    <tr>\n","      <th>1560</th>\n","      <td>509</td>\n","      <td>960.0</td>\n","      <td>960.0</td>\n","      <td>1.0</td>\n","      <td>0.633984</td>\n","      <td>0.204167</td>\n","      <td>0.019531</td>\n","      <td>0.058333</td>\n","    </tr>\n","    <tr>\n","      <th>1562</th>\n","      <td>1050</td>\n","      <td>960.0</td>\n","      <td>960.0</td>\n","      <td>0.0</td>\n","      <td>0.484375</td>\n","      <td>0.455556</td>\n","      <td>0.043750</td>\n","      <td>0.094444</td>\n","    </tr>\n","    <tr>\n","      <th>1563</th>\n","      <td>1050</td>\n","      <td>960.0</td>\n","      <td>960.0</td>\n","      <td>1.0</td>\n","      <td>0.536719</td>\n","      <td>0.141667</td>\n","      <td>0.017188</td>\n","      <td>0.080556</td>\n","    </tr>\n","    <tr>\n","      <th>1564</th>\n","      <td>1050</td>\n","      <td>960.0</td>\n","      <td>960.0</td>\n","      <td>1.0</td>\n","      <td>0.558984</td>\n","      <td>0.223611</td>\n","      <td>0.016406</td>\n","      <td>0.063889</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>92 rows × 8 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2a081a3d-36a5-4e28-996a-63b9224d7b0f')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-2a081a3d-36a5-4e28-996a-63b9224d7b0f button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-2a081a3d-36a5-4e28-996a-63b9224d7b0f');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["      image_id  width  height  label         x         y         w         h\n","0         1035  960.0   960.0    1.0  0.704688  0.522917  0.034375  0.098611\n","1         1035  960.0   960.0    1.0  0.551562  0.201389  0.045312  0.077778\n","111        659  960.0   960.0    1.0  0.629297  0.297917  0.049219  0.134722\n","112        659  960.0   960.0    1.0  0.791797  0.624306  0.061719  0.109722\n","141         17  960.0   960.0    1.0  0.424609  0.602083  0.069531  0.115278\n","...        ...    ...     ...    ...       ...       ...       ...       ...\n","1559       509  960.0   960.0    1.0  0.870703  0.431250  0.025781  0.087500\n","1560       509  960.0   960.0    1.0  0.633984  0.204167  0.019531  0.058333\n","1562      1050  960.0   960.0    0.0  0.484375  0.455556  0.043750  0.094444\n","1563      1050  960.0   960.0    1.0  0.536719  0.141667  0.017188  0.080556\n","1564      1050  960.0   960.0    1.0  0.558984  0.223611  0.016406  0.063889\n","\n","[92 rows x 8 columns]"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["id_lst1 = [1035, 1029, 1041, 1034, 1027, 1002, 1049, 1050, 1048]\n","id_lst2 = [650, 465, 498, 630, 657, 466, 642]\n","\n","final_df1 = val_df.loc[[True if id in id_lst1 else False for id in val_df['image_id']]]\n","final_df2 = val_df.loc[[True if id in id_lst2 else False for id in val_df['image_id']]]\n","\n","final_df = pd.concat((final_df1, final_df2), axis=0).reset_index(drop=True)\n","final_df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"2WdOM555L7ft","executionInfo":{"status":"ok","timestamp":1640177730514,"user_tz":-420,"elapsed":449,"user":{"displayName":"Hoang Pham Viet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjqSUz9UAYIe9qEo0tiprwACQv8lgsyFb8T2wzpcg=s64","userId":"13405977886381099576"}},"outputId":"27319293-dba6-448b-fd37-ee28cc0f41fe"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-7e5ca1c7-50e0-4b98-b95b-c029db0ff03c\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>image_id</th>\n","      <th>width</th>\n","      <th>height</th>\n","      <th>label</th>\n","      <th>x</th>\n","      <th>y</th>\n","      <th>w</th>\n","      <th>h</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1041</td>\n","      <td>960.0</td>\n","      <td>960.0</td>\n","      <td>0.0</td>\n","      <td>0.558984</td>\n","      <td>0.373611</td>\n","      <td>0.046094</td>\n","      <td>0.077778</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1050</td>\n","      <td>960.0</td>\n","      <td>960.0</td>\n","      <td>0.0</td>\n","      <td>0.484375</td>\n","      <td>0.455556</td>\n","      <td>0.043750</td>\n","      <td>0.094444</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1050</td>\n","      <td>960.0</td>\n","      <td>960.0</td>\n","      <td>1.0</td>\n","      <td>0.536719</td>\n","      <td>0.141667</td>\n","      <td>0.017188</td>\n","      <td>0.080556</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1050</td>\n","      <td>960.0</td>\n","      <td>960.0</td>\n","      <td>1.0</td>\n","      <td>0.558984</td>\n","      <td>0.223611</td>\n","      <td>0.016406</td>\n","      <td>0.063889</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1034</td>\n","      <td>960.0</td>\n","      <td>960.0</td>\n","      <td>1.0</td>\n","      <td>0.596484</td>\n","      <td>0.295833</td>\n","      <td>0.049219</td>\n","      <td>0.094444</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7e5ca1c7-50e0-4b98-b95b-c029db0ff03c')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-7e5ca1c7-50e0-4b98-b95b-c029db0ff03c button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-7e5ca1c7-50e0-4b98-b95b-c029db0ff03c');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["   image_id  width  height  label         x         y         w         h\n","0      1041  960.0   960.0    0.0  0.558984  0.373611  0.046094  0.077778\n","1      1050  960.0   960.0    0.0  0.484375  0.455556  0.043750  0.094444\n","2      1050  960.0   960.0    1.0  0.536719  0.141667  0.017188  0.080556\n","3      1050  960.0   960.0    1.0  0.558984  0.223611  0.016406  0.063889\n","4      1034  960.0   960.0    1.0  0.596484  0.295833  0.049219  0.094444"]},"metadata":{},"execution_count":28}]},{"cell_type":"markdown","source":["## Labels distribution of our data augmentation set"],"metadata":{"id":"aW6rL6N-9sj4"}},{"cell_type":"code","source":["import glob\n","import pandas as pd\n","import numpy as np\n","\n","def merge_per_folder(folder_path):\n","  \"\"\"Read all txt annotation files & return a dataframe containing them\n","  Input:\n","    folder_path : folder's path contained txt files\n","  Output:\n","    Name of the output file the merged lines will be written to.\n","  \"\"\"\n","\n","  train_csv = list()\n","  # make sure there's a slash to the folder path \n","  folder_path += \"\" if folder_path[-1] == \"/\" else \"/\"\n","  # get all text files\n","  txt_files = glob.glob(folder_path + \"*.txt\")\n","\n","  # Read each txt file\n","  for txt_file in txt_files:\n","    id = [txt_file.strip().split('/')[-1][:-4], 960.0, 960.0]\n","    # Read the content of file\n","    with open(txt_file, 'rt') as fd:\n","      lines = fd.readlines()\n","      for line in lines:\n","        box = line.strip().split(' ')\n","        train_csv.append(id+box)\n","  \n","  return train_csv\n","\n","aug_df = merge_per_folder(\"./dataset_aug/labels/train\")\n","aug_df = pd.DataFrame(aug_df, columns=['image_id', 'width', 'height', 'label', 'x', 'y', 'w', 'h'])\n","len(aug_df.image_id.unique())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BJ-zLla-9sdH","executionInfo":{"status":"ok","timestamp":1640099554586,"user_tz":-420,"elapsed":13588,"user":{"displayName":"Hoang Pham Viet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjqSUz9UAYIe9qEo0tiprwACQv8lgsyFb8T2wzpcg=s64","userId":"13405977886381099576"}},"outputId":"fde6a092-28c4-4878-a911-6f6ffa0dbf92"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2785"]},"metadata":{},"execution_count":46}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","dis = aug_df['label'].value_counts()\n","fig = plt.figure()\n","ax = fig.add_axes([0,0,1,1])\n","ax.bar(list(dis.index), dis.values)\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":336},"id":"e3wU6mo49sWp","executionInfo":{"status":"ok","timestamp":1640099554587,"user_tz":-420,"elapsed":5,"user":{"displayName":"Hoang Pham Viet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjqSUz9UAYIe9qEo0tiprwACQv8lgsyFb8T2wzpcg=s64","userId":"13405977886381099576"}},"outputId":"bbe0f2d9-b4de-4b00-82c1-8b44fac48f56"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"BLwYfrQQ9qrd"},"source":["## Copy train, val & public_test images as well as annotation text files from \"dataset\" folder to \"dataset_aug\" folder (Already done don't run this part!!!!!)"]},{"cell_type":"code","metadata":{"id":"n0IWQatN9qmV"},"source":["# DONE\n","def load_copy_images(start_folder, end_folder):\n","  \"\"\"Load all the .jpeg images from the start_folder & save them into end_folder\"\"\"\n","\n","  # make sure there's a slash to the folder path \n","  start_folder += \"\" if start_folder[-1] == \"/\" else \"/\"\n","  # get all text files\n","  img_files = glob.glob(start_folder + \"*.jpg\")\n","  a = 0\n","\n","  for img in img_files:\n","    # a += 1\n","    # if a > 200:\n","    #   break\n","\n","    # Load the images from start_folder (dataset)\n","    img_arr = cv2.imread(img, cv2.IMREAD_COLOR)\n","    assert img_arr is not None, 'Image Not Found ' + img\n","    img_arr = cv2.cvtColor(img_arr, cv2.COLOR_BGR2RGB)  # BGR to RGB\n","\n","\n","    # Extract the name (id) of images\n","    img_id = img.strip().split('/')[-1][:-4]\n","\n","    # Save images to end_folder (dataset_aug)\n","    im = Image.fromarray(img_arr, \"RGB\")\n","    im.save(f\"{end_folder}/{img_id}.jpg\")\n","    # print(f\"{end_folder}/{img_id}.jpg\")\n","\n","# Public_test image folder\n","# load_copy_images('./dataset_origin/images/public_test', './dataset_origin/images/val')\n","# load_copy_images('./dataset_origin/data_kaggle/images', './dataset_aug/images/train')\n","# load_copy_images('./dataset_origin/images/train', './dataset_aug/images/train')\n","\n","# Mosaic\n","# load_copy_images('./dataset_origin/data_aug/mosaic/images', './dataset_aug/images/train')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qZ7DuMy89qgH"},"source":["# DONE\n","def load_copy_anno_txt(start_folder, end_folder):\n","  \"\"\"Load & copy annotation txt files from start_folder (dataset) to end_folder (dataset_aug)\"\"\"\n","  # make sure there's a slash to the folder path \n","  start_folder += \"\" if start_folder[-1] == \"/\" else \"/\"\n","  # get all text files\n","  txt_files = glob.glob(start_folder + \"*.txt\")\n","  a = 0\n","\n","  for txt_f in txt_files:\n","    # Extract the name (id) of txt annotation file\n","    txt_id = txt_f.strip().split('/')[-1][:-4]\n","\n","    with open(txt_f, 'r') as fd:\n","      lines = fd.readlines()\n","      # Save the string to txt file in end_folder (dataset_aug)\n","      txt_file = open(f'{end_folder}/{txt_id}.txt', 'w')\n","      for line in lines:\n","        txt_file.write(line)  # Write each txt line into a new file\n","      txt_file.close()\n","    # print(f'{end_folder}/{txt_id}.txt')\n","\n","# public_test txt folder\n","# load_copy_anno_txt('./dataset_origin/labels/public_test', './dataset_origin/labels/val')\n","# load_copy_anno_txt('./dataset_origin/data_kaggle/labels', './dataset_aug/labels/train')\n","# load_copy_anno_txt('./dataset_origin/labels/train', './dataset_aug/labels/train')\n","\n","# Mosaic\n","# load_copy_anno_txt('./dataset_origin/data_aug/mosaic/labels', './dataset_aug/labels/train')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kHatDDl0BvZ4"},"source":["### Filter labels of all images contained incorrect_mask labels from .csv metadata files"]},{"cell_type":"code","metadata":{"id":"1Sve3yNt7E_T"},"source":["def filterIncorrectMask(metadata_df):\n","  \"\"\"\n","    Return metadata contained all image's id having incorrect_mask & other related label also\n","  \"\"\"\n","  # Filter only label 2 \"incorrect_mask\"\n","  incorr = metadata_df.loc[metadata_df['label'] != 2]\n","  # Image's id contained label 2\n","  unique_id = incorr['image_id'].unique()\n","  # Filter all images contained incorrect_mask\n","  incorr_mask = metadata_df.loc[[True if id in unique_id else False for id in metadata_df['image_id']]]\n","\n","  return incorr_mask.reset_index(drop=True)\n","\n","# train_df = pd.read_csv('./dataset_origin/train_csv.csv', index_col=False)\n","# mask_no_mask = filterIncorrectMask(train_df)\n","\n","# val_df = pd.read_csv('./dataset_origin/val_csv.csv', index_col=False)\n","# incorr_mask_val = filterIncorrectMask(val_df)\n","\n","# test_df = pd.read_csv('./dataset_origin/test_csv.csv', index_col=False)\n","# incorr_mask_test = filterIncorrectMask(test_df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AbLiqvSP9qU9"},"source":["# DONE\n","def metadata_to_images(metadata_df, start_folder, end_folder):\n","  \"\"\"Load the images based on metadata file\"\"\"\n","  # make sure there's a slash to the folder path \n","  start_folder += \"\" if start_folder[-1] == \"/\" else \"/\"\n","  # Filter image id\n","  img_ids = metadata_df['image_id'].unique()\n","\n","  # get all text files\n","  img_files = list()\n","  for id in img_ids:\n","    img_files.append(os.path.join(start_folder, str(id)+\".jpg\"))\n","\n","  for img in img_files:\n","    # Load the images from start_folder (dataset)\n","    img_arr = cv2.imread(img, cv2.IMREAD_COLOR)\n","    assert img_arr is not None, 'Image Not Found '\n","    img_arr = cv2.cvtColor(img_arr, cv2.COLOR_BGR2RGB)  # BGR to RGB\n","\n","    # Extract the name (id) of images\n","    img_id = img.strip().split('/')[-1][:-4]\n","\n","    # Save images to end_folder (dataset_aug)\n","    im = Image.fromarray(img_arr, \"RGB\")\n","    im.save(f\"{end_folder}/{img_id}.jpg\")\n","    # print(f\"{end_folder}/{img_id}.jpg\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DgWb5vG5BXjM"},"source":["# DONE\n","def metadata_to_txt(metadata_df, start_folder, end_folder):\n","  \"\"\"Load & copy annotation txt files from start_folder (dataset) to end_folder (dataset_aug)\"\"\"\n","  # make sure there's a slash to the folder path \n","  start_folder += \"\" if start_folder[-1] == \"/\" else \"/\"\n","\n","  # Filter image id\n","  txt_ids = metadata_df['image_id'].unique()\n","\n","  # get all text files\n","  txt_files = list()\n","  for id in txt_ids:\n","    txt_files.append(os.path.join(start_folder, str(id)+\".txt\"))\n","\n","  for txt_f in txt_files:\n","    # Extract the name (id) of txt annotation file\n","    txt_id = txt_f.strip().split('/')[-1][:-4]\n","\n","    with open(txt_f, 'r') as fd:\n","      lines = fd.readlines()\n","      # Save the string to txt file in end_folder (dataset_aug)\n","      txt_file = open(f'{end_folder}/{txt_id}.txt', 'w')\n","      for line in lines:\n","        txt_file.write(line)  # Write each txt line into a new file\n","      txt_file.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wx828gDZBXdD"},"source":["# metadata_to_images(incorr_val, './dataset_origin/images/val', './dataset_origin/images/train')\n","# metadata_to_txt(incorr_val, './dataset_origin/labels/val', './dataset_origin/labels/train')\n","\n","# metadata_to_images(incorr_mask_df, './dataset_origin/images/train', './dataset_aug/images/train')\n","# metadata_to_txt(incorr_mask_df, './dataset_origin/labels/train', './dataset_aug/labels/train')\n","\n","# metadata_to_images(incorr_mask_test, './dataset_origin/images/public_test', './dataset_origin/images/val')\n","# metadata_to_txt(incorr_mask_test, './dataset_origin/labels/public_test', './dataset_origin/labels/val')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wBpyVHQ1cJHc"},"source":["# DONE\n","def pattern_copy_images(start_folder, end_folder, pattern):\n","  \"\"\"Load all the .jpeg images from the start_folder & save them into end_folder\"\"\"\n","\n","  # make sure there's a slash to the folder path \n","  start_folder += \"\" if start_folder[-1] == \"/\" else \"/\"\n","  # get all text files\n","  img_files = glob.glob(start_folder + \"*.jpg\")\n","\n","  for img in img_files:\n","    # Load the images from start_folder (dataset)\n","    img_arr = cv2.imread(img, cv2.IMREAD_COLOR)\n","    assert img_arr is not None, 'Image Not Found ' + imgpath\n","    img_arr = cv2.cvtColor(img_arr, cv2.COLOR_BGR2RGB)  # BGR to RGB\n","\n","    # Extract the name (id) of images\n","    img_id = img.strip().split('/')[-1][:-4]\n","    # Filter only image with pattern's name\n","    result = re.findall(pattern, img_id)\n","\n","    if len(result):\n","      # Save images to end_folder (dataset_aug)\n","      im = Image.fromarray(img_arr, \"RGB\")\n","      print(im, f\"{end_folder}/{result[0]}.jpg\")\n","      im.save(f\"{end_folder}/{result[0]}.jpg\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fXITt2X68iJn"},"source":["## Delete files based on list of id "]},{"cell_type":"code","metadata":{"id":"mTnMEnhE8h1h"},"source":["# DELETE all the added images & annotation files from train_val folder\n","def delete_files_list(folder_path, indx_lst):\n","  \"\"\"Delete all img_{}.jpeg or img_{}.txt files from a given folder base on the name's pattern\"\"\"\n","  a = 0\n","  # # (OPTION 1) DELETE IMAGE\n","  # # make sure there's a slash to the folder path \n","  # folder_path += \"\" if folder_path[-1] == \"/\" else \"/\"\n","  # # get all text files\n","  # img_files = glob.glob(folder_path + \"*.jpg\")\n","\n","  # for img_f in img_files:\n","  #   # Extract the name (id) of txt annotation file\n","  #   img_f = img_f.strip().split('/')[-1][:-4]\n","\n","  #   if int(img_f) in indx_lst:\n","  #     # Remove file from folder\n","  #     os.remove(f\"{folder_path}/{img_f}.jpg\")\n","  #     # print(f\"{folder_path}/{img_f}.jpg\")\n","\n","\n","  # (OPTION 2) DELETE TXT FILE\n","  # make sure there's a slash to the folder path \n","  folder_path += \"\" if folder_path[-1] == \"/\" else \"/\"\n","  # get all text files\n","  txt_files = glob.glob(folder_path + \"*.txt\")\n","\n","  for txt_f in txt_files:\n","    # Extract the name (id) of txt annotation file\n","    txt_f = txt_f.strip().split('/')[-1][:-4]\n","\n","    if int(txt_f) in indx_lst:\n","      a += 1\n","      # Remove file from folder\n","      os.remove(f\"{folder_path}{txt_f}.txt\")\n","      # print(f\"{folder_path}{txt_f}.txt\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U47lG1vh9bpd"},"source":["# delete_files_list(\"./dataset_origin/images/val\", incorr_lst)\n","# delete_files_list(\"./dataset_origin/labels/val\", incorr_lst)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Alo5KNLPTlOP"},"source":["## Delete files based on pattern"]},{"cell_type":"code","metadata":{"id":"-8sbUqUjTlID"},"source":["import glob\n","import re\n","\n","def delete_files_base_pattern(folder_path, pattern):\n","  \"\"\"Delete all img_{}.jpeg or img_{}.txt files from a given folder base on the name's pattern\"\"\"\n","  a = 0\n","  # # (OPTION 1) DELETE IMAGE\n","  # # make sure there's a slash to the folder path \n","  # folder_path += \"\" if folder_path[-1] == \"/\" else \"/\"\n","  # # get all text files\n","  # img_files = glob.glob(folder_path + \"*.jpg\")\n","\n","  # for img_f in img_files:\n","  #   # Extract the name (id) of txt annotation file\n","  #   img_f = img_f.strip().split('/')[-1][:-4]\n","  #   # Filter only image with name \"img_{}.jpeg\"\n","  #   result = re.findall(pattern, img_f)\n","  #   if len(result):\n","  #     # Remove file from folder\n","  #     os.remove(folder_path+result[0]+'.jpg')\n","  #     # print(folder_path+result[0]+'.jpg')\n","\n","\n","  # (OPTION 2) DELETE TXT FILE\n","  # make sure there's a slash to the folder path \n","  folder_path += \"\" if folder_path[-1] == \"/\" else \"/\"\n","  # get all text files\n","  txt_files = glob.glob(folder_path + \"*.txt\")\n","\n","  for txt_f in txt_files:\n","    # Extract the name (id) of txt annotation file\n","    txt_f = txt_f.strip().split('/')[-1][:-4]\n","    # Filter only image with pattern's name\n","    result = re.findall(pattern, txt_f)\n","    if len(result):\n","      a += 1\n","      # Remove file from folder\n","      os.remove(folder_path+result[0]+'.txt')\n","      # print(folder_path+result[0]+'.txt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bQurpd9vTlB_"},"source":["# delete_files_base_pattern(\"./dataset_aug/images/train/\", r\"img_rotateShear_.*\")\n","# delete_files_base_pattern(\"./dataset_aug/labels/train/\", r\"img_rotateShear_.*\")\n","\n","# delete_files_base_pattern(\"./dataset_aug/labels/train/\", r\"rsbbsc_.*\")\n","# delete_files_base_pattern(\"./dataset_aug/images/train/\", r\"rsbbsc_.*\")\n","\n","# delete_files_base_pattern(\"./dataset_aug/images/train/\", r\"img_cutout_.*\")\n","# delete_files_base_pattern(\"./dataset_aug/labels/train/\", r\"img_cutout_.*\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rFIvC8xV2ujX"},"source":["## Split up \"train\" and \"val\" dataset"]},{"cell_type":"code","metadata":{"id":"jHS5hLZ587VC"},"source":["def split_images(start_folder, end_folder, file_ids):\n","  \"\"\"Load all the .jpeg images from the start_folder & save them into end_folder\"\"\"\n","  # make sure there's a slash to the folder path \n","  start_folder += \"\" if start_folder[-1] == \"/\" else \"/\"\n","  # get all text files\n","  img_files = glob.glob(start_folder + \"*.jpg\")\n","\n","  for img in img_files:\n","    # Extract the name (id) of images\n","    img_id = img.strip().split('/')[-1][:-4]\n","\n","    if int(img_id) in list(file_ids):\n","      # Load the images from start_folder (dataset)\n","      img_arr = cv2.imread(img, cv2.IMREAD_COLOR)\n","      assert img_arr is not None, 'Image Not Found ' + imgpath\n","      img_arr = cv2.cvtColor(img_arr, cv2.COLOR_BGR2RGB)  # BGR to RGB\n","\n","      # Add images to end folder\n","      im = Image.fromarray(img_arr, \"RGB\")\n","      im.save(f\"{end_folder}/{img_id}.jpg\")\n","      # print(f\"{end_folder}/{img_id}.jpg\")\n","    else:\n","      continue"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yepdM9BB874k"},"source":["# split_images('./dataset_origin/images/train_val', './dataset_origin/images/train', train_ids)\n","# split_images('./dataset_origin/images/train_val', './dataset_origin/images/val', val_ids)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dLmiNjJN879I"},"source":["# DONE\n","def split_txt(start_folder, end_folder, file_ids):\n","  \"\"\"Load & copy annotation txt files from start_folder (dataset) to end_folder (dataset_aug)\"\"\"\n","  # make sure there's a slash to the folder path \n","  start_folder += \"\" if start_folder[-1] == \"/\" else \"/\"\n","  # get all text files\n","  txt_files = glob.glob(start_folder + \"*.txt\")\n","  for txt_f in txt_files:\n","    # Extract the name (id) of txt annotation file\n","    txt_id = int(txt_f.strip().split('/')[-1][:-4])\n","\n","    if int(txt_id) in list(file_ids):\n","      with open(txt_f, 'r') as fd:\n","        lines = fd.readlines()\n","        # Save the string to txt file in end_folder (dataset_aug)\n","        txt_file = open(f'{end_folder}/{txt_id}.txt', 'w')\n","        for line in lines:\n","          txt_file.write(line)  # Write each txt line into a new file\n","        txt_file.close()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"amUF_g6KMUtc"},"source":["# public_test txt folder\n","# split_txt('./dataset_origin/labels/train_val', './dataset_origin/labels/train', train_ids)\n","# split_txt('./dataset_origin/labels/train_val', './dataset_origin/labels/val', val_ids)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VUXV8v4b5yKW"},"source":["# FPT dataset creation\n","Great reference notebook: https://www.kaggle.com/sreevishnudamodaran/effdet-pytorch-cutmix-mixup-kfold-cosanneal"]},{"cell_type":"code","metadata":{"id":"hu07k2Nb5yQC"},"source":["from sklearn.utils import shuffle\n","import random\n","from torch.utils.data import DataLoader, Dataset\n","\n","class FPTDataset(Dataset):\n","  def __init__(self, dataframe, image_dir, transforms=None):\n","    super().__init__()\n","\n","    self.df = dataframe  # Annotation & Image's ID dataframe\n","    self.transforms = transforms  # Albumentation's augmentation\n","    self.image_ids = shuffle(dataframe['image_id'].unique())  # Image's ID\n","    self.labels = [np.zeros((0, 4), dtype=np.float32)] * len(self.image_ids) # Image's bboxes\n","    self.class_labels = [np.zeros((0, 1), dtype=np.float32)] * len(self.image_ids)  # Image's label\n","    self.image_dir = image_dir\n","    im_w = 1280\n","    im_h = 720\n","\n","    # Loop through each image (Each image might containt multiple bboxes & labels)\n","    for i, img_id in enumerate(self.image_ids):\n","      records = self.df[self.df['image_id'] == img_id]\n","      boxes = records[['x', 'y', 'w', 'h']].values  # Annotations\n","      class_label = records[['label']].values\n","      self.labels[i] = np.array(boxes)\n","      self.class_labels[i] = class_label\n","\n","\n","  def __getitem__(self, index: int):\n","    # DATA AUGMENTATION\n","    if self.transforms is not None:\n","      # Load image\n","      image, (h0, w0) = load_image(self, index)\n","\n","      # Augmentation\n","      augmented = self.transforms(image=image, bboxes=self.labels[index], class_labels=self.class_labels[index])\n","      image = augmented['image']\n","      image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # BGR to RGB\n","      bboxes = augmented['bboxes']\n","      labels = augmented['class_labels']\n","      return image, bboxes, labels\n","\n","  def __len__(self) -> int:\n","    return self.image_ids.shape[0]\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dL16cd177G5N"},"source":["# Helper functions"]},{"cell_type":"code","metadata":{"id":"K_q2jHtg5yVF"},"source":["def load_image(self, index):\n","  # loads 1 image from dataset, returns img, original hw, resized hw\n","  \"\"\"Load 1 image from dataset\n","  Input:\n","    index: idx to search for image's id\n","  Output:\n","    img, hw_original, hw_resized \"\"\"\n","  # Read an image using opencv2\n","  image_id = self.image_ids[index]\n","  img = cv2.imread(f'{self.image_dir}/{image_id}.jpg', cv2.IMREAD_COLOR)\n","    \n","  assert img is not None, 'Image Not Found ' + imgpath\n","  h0, w0 = img.shape[:2]  # orig hw\n","  return img, (h0, w0)  # img, hw_original\n","\n","def yolo_to_coco(self, bbox, orig_w, orig_h):\n","  bbox[:, 2] = bbox[:, 2]*orig_w\n","  bbox[:, 3] = bbox[:, 3]*orig_h\n","  bbox[:, 1] = bbox[:, 1]*orig_h - (bbox[:, 3]/2)\n","  bbox[:, 0] = bbox[:, 0]*orig_w - (bbox[:, 2]/2)\n","  return bbox"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yqn5B0wr-wX9"},"source":["def arr_to_str(arr, count_f):\n","  \"\"\"Transform arr to string & save them to folder dataset_aug\"\"\"\n","  result = str()\n","\n","  # Arr to String\n","  label_1 = int(arr[0])\n","  result = str(label_1) + ' '\n","  line = ' '.join([str(item) for item in arr[1:]]) + '\\n'\n","  result = result + line\n","  return result\n","\n","\n","def pascal_to_yolo(xmin, ymin, xmax, ymax, image_width=640, image_height=640):\n","  x_coord = (xmin + xmax) / 2 / image_width\n","  y_coord = (ymin + ymax) / 2 / image_height\n","  shape_width = (xmax - xmin) / image_width\n","  shape_height = (ymax - ymin) / image_height\n","  return x_coord, y_coord, shape_width, shape_height\n","\n","\n","def yolo_to_pascal(x, y, w, h, width, height):\n","  xmax = int((x*width) + (w * width)/2.0)\n","  xmin = int((x*width) - (w * width)/2.0)\n","  ymax = int((y*height) + (h * height)/2.0)\n","  ymin = int((y*height) - (h * height)/2.0)\n","  return xmin, ymin, xmax, ymax"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YhucFNinRtOe"},"source":["import random, math\n","\n","\n","def collate_fn(batch):\n","    return tuple(zip(*batch))\n","\n","def aug_visualize(train_val_df, train_val_dir, transform):\n","  train_dataset = FPTDataset(train_val_df, train_val_dir, transform)  # 792 images\n","  train_data_loader = DataLoader(\n","      train_dataset,\n","      batch_size=9,\n","      shuffle=True,\n","      num_workers=4,\n","      collate_fn=collate_fn\n","  )\n","\n","\n","  fig, ax = plt.subplots(3, 3, figsize=(25, 15))\n","  ax = ax.flatten()\n","  images, targets, class_labels = next(iter(train_data_loader))\n","\n","  for i in range(9):  # Go through each batch \n","    boxes = targets[i]\n","    sample = images[i]\n","    height, width = sample.shape[:2]\n","\n","    for box in boxes:\n","      # Convert from Yolo to Pascal_voc\n","      box = yolo_to_pascal(box[0], box[1], box[2], box[3], width, height)\n","      # Drawing bounding box\n","      cv2.rectangle(sample,\n","                (int(box[0]), int(box[1])),\n","                (int(box[2]), int(box[3])),\n","                220, 3)\n","\n","    ax[i].imshow(sample)  # Visualize a sample for each batch\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"59Osx3tKR0kM"},"source":["## Experiment 13 (Total < 3000 images)"]},{"cell_type":"code","metadata":{"id":"w3EzHHYfNBCi"},"source":["def RotateShear():\n","  return A.Compose([\n","      A.OneOf([\n","               A.augmentations.geometric.rotate.Rotate(limit=10, p=1.0),\n","               A.augmentations.geometric.transforms.Affine(shear=[-10,10], interpolation=1, p=1.0)\n","      ]),\n","      A.augmentations.transforms.HorizontalFlip(p=0.5),\n","      A.augmentations.geometric.resize.Resize(720, 1280, interpolation=1, always_apply=True, p=1.0),\n","  ], p=1.0, \n","  # This \"format\" here is for coordinate of the input\n","  bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qZbtWKBVNA9a"},"source":["# Visualize augmentation images\n","rotate_shear = RotateShear()\n","aug_visualize(diff_mask_df, DIR_TRAIN, rotate_shear)\n","\n","# UNCOMMENT TO SAVE AUGMENTED IMAGES INTO FOLDER (.JPG + .TXT)\n","# train_dataset = FPTDataset(diff_mask_df, DIR_TRAIN, RotateShear())  # 792 images\n","# train_data_loader = DataLoader(\n","#     train_dataset,\n","#     batch_size=25,\n","#     shuffle=True,\n","#     num_workers=4,\n","#     collate_fn=collate_fn\n","# )\n","\n","# a1 = 0\n","# b1 = 0\n","\n","# # TRY GENERATE 50 AUGMENTED gray IMAGES & SAVE THEM TO \"dataset_aug\" FODLER \n","# for i in range(2):\n","#   # Image & labels after data augmentation\n","#   imgs, targets, class_labels = next(iter(train_data_loader))\n","\n","#   for bboxes, class_label in zip(targets, class_labels):\n","#     a1 += 1\n","\n","#     txt_file = open(f'./dataset_aug/labels/train/img_rotateShear_diffmask_{a1}.txt', 'w')\n","#     # Through each bbox of an image\n","#     for j in range(len(bboxes)):\n","#       # Normalize the box's annotation after augmentation (AS requirement from competition)\n","#       a,b,c,d = bboxes[j][0], bboxes[j][1], bboxes[j][2], bboxes[j][3]\n","#       label_yolo = np.array([class_label[j][0], a, b, c, d])\n","#       label_yolo = arr_to_str(label_yolo, a1)\n","\n","#       # Save the string for txt file\n","#       txt_file.write(label_yolo)\n","#     txt_file.close()\n","    \n","\n","#   # Save images into folder \"images/train\"\n","#   for img in imgs:\n","#     b1 += 1\n","#     im = Image.fromarray(img, \"RGB\")\n","#     im.save(f'./dataset_aug/images/train/img_rotateShear_diffmask_{b1}.jpg')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R7hfeAGJmUJl"},"source":["def heavyAugment():\n","  return A.Compose([\n","      A.OneOf([\n","               A.augmentations.transforms.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, \n","                                                            val_shift_limit=0.2, p=0.9),\n","               A.augmentations.transforms.RandomBrightnessContrast (brightness_limit=0.2, contrast_limit=0.2, \n","                                                                    brightness_by_max=True, always_apply=False, p=0.9),\n","      ]),\n","      A.OneOf([\n","               A.augmentations.transforms.GaussianBlur (blur_limit=(3, 5), sigma_limit=0, always_apply=False, p=0.7),\n","               A.augmentations.transforms.MotionBlur(blur_limit=(3, 5), p=0.7),\n","      ]),\n","      A.augmentations.transforms.CLAHE (clip_limit=(1, 2), tile_grid_size=(8, 8), always_apply=False, p=0.5),\n","      A.augmentations.geometric.resize.Resize(720, 1280, interpolation=1, always_apply=True, p=1.0),\n","  ], p=1.0, \n","  # This \"format\" here is for coordinate of the input\n","  bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CAXQ5XnMmaeb"},"source":["heavy_aug = heavyAugment()\n","aug_visualize(train_val_df, DIR_TRAIN_VAL, heavy_aug)\n","\n","# UNCOMMENT TO SAVE AUGMENTED IMAGES INTO FOLDER (.JPG + .TXT)\n","# train_dataset = FPTDataset(train_val_df, DIR_TRAIN_VAL, heavyAugment())  # 792 images\n","# train_data_loader = DataLoader(\n","#     train_dataset,\n","#     batch_size=25,\n","#     shuffle=True,\n","#     num_workers=4,\n","#     collate_fn=collate_fn\n","# )\n","\n","# a1 = 0\n","# b1 = 0\n","\n","# # TRY GENERATE 600 AUGMENTED gray IMAGES & SAVE THEM TO \"dataset_aug\" FODLER \n","# for i in range(24):\n","#   # Image & labels after data augmentation\n","#   imgs, targets, class_labels = next(iter(train_data_loader))\n","\n","#   for bboxes, class_label in zip(targets, class_labels):\n","#     a1 += 1\n","\n","#     txt_file = open(f'./dataset_aug/labels/train/img_heavyAug_{a1}.txt', 'w')\n","#     # Through each bbox of an image\n","#     for j in range(len(bboxes)):\n","#       # Normalize the box's annotation after augmentation (AS requirement from competition)\n","#       a,b,c,d = bboxes[j][0], bboxes[j][1], bboxes[j][2], bboxes[j][3]\n","#       label_yolo = np.array([class_label[j][0], a, b, c, d])\n","#       label_yolo = arr_to_str(label_yolo, a1)\n","\n","#       # Save the string for txt file\n","#       txt_file.write(label_yolo)\n","#     txt_file.close()\n","    \n","\n","#   # Save images into folder \"images/train\"\n","#   for img in imgs:\n","#     b1 += 1\n","#     im = Image.fromarray(img, \"RGB\")\n","#     im.save(f'./dataset_aug/images/train/img_heavyAug_{b1}.jpg')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1YH6PQK1bwzj"},"source":["def cutOut():\n","  return A.Compose([\n","      A.Cutout(num_holes=6, max_h_size=32, max_w_size=32, fill_value=0, p=1.0),\n","      A.augmentations.transforms.HorizontalFlip(p=0.8),\n","      A.augmentations.geometric.resize.Resize(720, 1280, interpolation=1, always_apply=False, p=1.0),\n","  ], p=1.0, \n","  # This \"format\" here is for coordinate of the input\n","  bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1mrm2v1sbwT6","executionInfo":{"status":"ok","timestamp":1640178603179,"user_tz":-420,"elapsed":3489,"user":{"displayName":"Hoang Pham Viet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjqSUz9UAYIe9qEo0tiprwACQv8lgsyFb8T2wzpcg=s64","userId":"13405977886381099576"}},"outputId":"dea06b85-9cd5-475c-a451-6999200e2982"},"source":["cutout = cutOut()\n","aug_visualize(final_df, DIR_VAL, cutout)\n","\n","# UNCOMMENT TO SAVE AUGMENTED IMAGES INTO FOLDER (.JPG + .TXT)\n","# train_dataset = FPTDataset(final_df, DIR_VAL, cutOut())  # 792 images\n","# train_data_loader = DataLoader(\n","#       train_dataset,\n","#       batch_size=25,\n","#       shuffle=True,\n","#       num_workers=4,\n","#       collate_fn=collate_fn\n","# )\n","\n","# a1 = 0\n","# b1 = 0\n","\n","# # TRY GENERATE 50 AUGMENTED RANDOM TONE CURVE IMAGES & SAVE THEM TO \"dataset_aug\" FODLER \n","# for i in range(2):\n","#   # Image & labels after data augmentation\n","#   imgs, targets, class_labels = next(iter(train_data_loader))\n","\n","#   for bboxes, class_label in zip(targets, class_labels):\n","#     a1 += 1\n","\n","#     txt_file = open(f'./dataset_aug/labels/train/img_cutout_final_{a1}.txt', 'w')\n","#     # Through each bbox of an image\n","#     for j in range(len(bboxes)):\n","#       # Normalize the box's annotation after augmentation (AS requirement from competition)\n","#       a,b,c,d = bboxes[j][0], bboxes[j][1], bboxes[j][2], bboxes[j][3]\n","#       label_yolo = np.array([class_label[j][0], a, b, c, d])\n","#       label_yolo = arr_to_str(label_yolo, a1)\n","\n","#       # Save the string for txt file\n","#       txt_file.write(label_yolo)\n","#     txt_file.close()\n","    \n","\n","#   # Save images into folder \"images/train\"\n","#   for img in imgs:\n","#     b1 += 1\n","#     im = Image.fromarray(img, \"RGB\")\n","#     im.save(f'./dataset_aug/images/train/img_cutout_final_{b1}.jpg')\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/albumentations/augmentations/transforms.py:691: FutureWarning: This class has been deprecated. Please use CoarseDropout\n","  FutureWarning,\n"]}]},{"cell_type":"code","metadata":{"id":"TmeB_nhVma78"},"source":["def toGray():\n","  return A.Compose([\n","      A.augmentations.transforms.ToGray(p=1.0),\n","      # A.augmentations.transforms.HorizontalFlip(p=1.0),\n","      A.augmentations.geometric.resize.Resize(720, 1280, interpolation=1, always_apply=False, p=1.0),\n","  ], p=1.0, \n","  # This \"format\" here is for coordinate of the input\n","  bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hvAgvxSSmbAA"},"source":["togray = toGray()\n","aug_visualize(train_df, DIR_TRAIN, togray)\n","\n","# UNCOMMENT TO SAVE AUGMENTED IMAGES INTO FOLDER (.JPG + .TXT)\n","# train_dataset = FPTDataset(train_df, DIR_TRAIN, toGray())  # 792 images\n","# train_data_loader = DataLoader(\n","#       train_dataset,\n","#       batch_size=25,\n","#       shuffle=True,\n","#       num_workers=4,\n","#       collate_fn=collate_fn\n","# )\n","\n","# a1 = 0\n","# b1 = 0\n","\n","# # TRY GENERATE 100 AUGMENTED horizontal flip IMAGES & SAVE THEM TO \"dataset_aug\" FODLER \n","# for i in range(4):\n","#   # Image & labels after data augmentation\n","#   imgs, targets, class_labels = next(iter(train_data_loader))\n","\n","#   for bboxes, class_label in zip(targets, class_labels):\n","#     a1 += 1\n","\n","#     txt_file = open(f'./dataset_aug/labels/train/img_gray_{a1}.txt', 'w')\n","#     # Through each bbox of an image\n","#     for j in range(len(bboxes)):\n","#       # Normalize the box's annotation after augmentation (AS requirement from competition)\n","#       a,b,c,d = bboxes[j][0], bboxes[j][1], bboxes[j][2], bboxes[j][3]\n","#       label_yolo = np.array([class_label[j][0], a, b, c, d])\n","#       label_yolo = arr_to_str(label_yolo, a1)\n","\n","#       # Save the string for txt file\n","#       txt_file.write(label_yolo)\n","#     txt_file.close()\n","    \n","\n","#   # Save images into folder \"images/train\"\n","#   for img in imgs:\n","#     b1 += 1\n","#     im = Image.fromarray(img, \"RGB\")\n","#     im.save(f'./dataset_aug/images/train/img_gray_{b1}.jpg')\n","\n","# print(a1, b1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-nLIzk2JHTP6"},"source":["def HorFlip():\n","  return A.Compose([\n","      A.augmentations.transforms.HorizontalFlip(p=0.7),\n","      A.augmentations.geometric.resize.Resize(720, 960, interpolation=1, always_apply=False, p=1.0),\n","  ], p=1.0, \n","  # This \"format\" here is for coordinate of the input\n","  bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5n9j2F9FHTI2"},"source":["# HORIZONTAL FLIP ONLY IMAGES CONTAINED INCORRECT_MASK LABEL\n","horFlip = HorFlip()\n","aug_visualize(need_aug_df, DIR_TRAIN, horFlip)\n","\n","# UNCOMMENT TO SAVE AUGMENTED IMAGES INTO FOLDER (.JPG + .TXT)\n","# train_dataset = FPTDataset(need_aug_df, DIR_TRAIN, HorFlip())  # 792 images\n","# train_data_loader = DataLoader(\n","#       train_dataset,\n","#       batch_size=25,\n","#       shuffle=True,\n","#       num_workers=4,\n","#       collate_fn=collate_fn\n","# )\n","\n","# a1 = 0\n","# b1 = 0\n","\n","# # TRY GENERATE 100 AUGMENTED horizontal flip IMAGES & SAVE THEM TO \"dataset_aug\" FODLER \n","# for i in range(4):\n","#   # Image & labels after data augmentation\n","#   imgs, targets, class_labels = next(iter(train_data_loader))\n","\n","#   for bboxes, class_label in zip(targets, class_labels):\n","#     a1 += 1\n","\n","#     txt_file = open(f'./dataset_aug/labels/train/img_hflip_needed_{a1}.txt', 'w')\n","#     # Through each bbox of an image\n","#     for j in range(len(bboxes)):\n","#       # Normalize the box's annotation after augmentation (AS requirement from competition)\n","#       a,b,c,d = bboxes[j][0], bboxes[j][1], bboxes[j][2], bboxes[j][3]\n","#       label_yolo = np.array([class_label[j][0], a, b, c, d])\n","#       label_yolo = arr_to_str(label_yolo, a1)\n","\n","#       # Save the string for txt file\n","#       txt_file.write(label_yolo)\n","#     txt_file.close()\n","    \n","\n","#   # Save images into folder \"images/train\"\n","#   for img in imgs:\n","#     b1 += 1\n","#     im = Image.fromarray(img, \"RGB\")\n","#     im.save(f'./dataset_aug/images/train/img_hflip_needed_{b1}.jpg')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_7p5-5FEd6M7"},"source":["def hsvOrRandBrightContrast():\n","  return A.Compose([\n","      A.OneOf([\n","          A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit= 0.2, val_shift_limit=0.2, p=0.9),\n","          A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.9)], p=1.0),\n","      A.augmentations.geometric.resize.Resize(720, 1280, interpolation=1, always_apply=False, p=1.0),\n","  ], p=1.0, \n","  # This \"format\" here is for coordinate of the input\n","  bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# HSV or Random Brightness Contrast\n","horbc = hsvOrRandBrightContrast()\n","aug_visualize(train_df, DIR_TRAIN, horbc)\n","\n","# UNCOMMENT TO SAVE AUGMENTED IMAGES INTO FOLDER (.JPG + .TXT)\n","# train_dataset = FPTDataset(train_df, DIR_TRAIN, hsvOrRandBrightContrast())  # 792 images\n","# train_data_loader = DataLoader(\n","#       train_dataset,\n","#       batch_size=25,\n","#       shuffle=True,\n","#       num_workers=4,\n","#       collate_fn=collate_fn\n","# )\n","\n","# a1 = 0\n","# b1 = 0\n","\n","# # TRY GENERATE 100 AUGMENTED horizontal flip IMAGES & SAVE THEM TO \"dataset_aug\" FODLER \n","# for i in range(4):\n","#   # Image & labels after data augmentation\n","#   imgs, targets, class_labels = next(iter(train_data_loader))\n","\n","#   for bboxes, class_label in zip(targets, class_labels):\n","#     a1 += 1\n","\n","#     txt_file = open(f'./dataset_aug/labels/train/img_hsvOrBC_{a1}.txt', 'w')\n","#     # Through each bbox of an image\n","#     for j in range(len(bboxes)):\n","#       # Normalize the box's annotation after augmentation (AS requirement from competition)\n","#       a,b,c,d = bboxes[j][0], bboxes[j][1], bboxes[j][2], bboxes[j][3]\n","#       label_yolo = np.array([class_label[j][0], a, b, c, d])\n","#       label_yolo = arr_to_str(label_yolo, a1)\n","\n","#       # Save the string for txt file\n","#       txt_file.write(label_yolo)\n","#     txt_file.close()\n","    \n","\n","#   # Save images into folder \"images/train\"\n","#   for img in imgs:\n","#     b1 += 1\n","#     im = Image.fromarray(img, \"RGB\")\n","#     im.save(f'./dataset_aug/images/train/img_hsvOrBC_{b1}.jpg')"],"metadata":{"id":"OUf8didyjI2B"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B13l8RejmbDp"},"source":["def RandomSizedBBoxSafeCrop():\n","  return A.Compose([\n","      A.RandomSizedBBoxSafeCrop(720, 1280, erosion_rate=0, interpolation=cv2.INTER_AREA, p=1.0),\n","      # A.augmentations.geometric.resize.Resize(720, 1280, interpolation=1, always_apply=False, p=1.0),\n","  ], p=1.0, \n","  # This \"format\" here is for coordinate of the input\n","  bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eLQxKlPhmbJZ"},"source":["rsbbsc = RandomSizedBBoxSafeCrop()\n","aug_visualize(train_df, DIR_TRAIN, rsbbsc)\n","\n","# UNCOMMENT TO SAVE AUGMENTED IMAGES INTO FOLDER (.JPG + .TXT)\n","# train_dataset = FPTDataset(train_df, DIR_TRAIN, RandomSizedBBoxSafeCrop())\n","# train_data_loader = DataLoader(\n","#     train_dataset,\n","#     batch_size=25,\n","#     shuffle=True,\n","#     num_workers=4,\n","#     collate_fn=collate_fn\n","# )\n","\n","# a1 = 0\n","# b1 = 0\n","\n","# # TRY GENERATE 600 AUGMENTED sharpen IMAGES & SAVE THEM TO \"dataset_aug\" FODLER \n","# for i in range(24):\n","#   # Image & labels after data augmentation\n","#   imgs, targets, class_labels = next(iter(train_data_loader))\n","\n","#   for bboxes, class_label in zip(targets, class_labels):\n","#     a1 += 1\n","\n","#     # txt_file = open(f'./dataset_aug/labels/train/rsbbsc_{a1}.txt', 'w')\n","#     txt_file = open(f'./dataset_origin/data_aug/randsafecrop/labels/rsbbsc_{a1}.txt', 'w')\n","#     # Through each bbox of an image\n","#     for j in range(len(bboxes)):\n","#       # Normalize the box's annotation after augmentation (AS requirement from competition)\n","#       a,b,c,d = bboxes[j][0], bboxes[j][1], bboxes[j][2], bboxes[j][3]\n","#       label_yolo = np.array([class_label[j][0], a, b, c, d])\n","#       label_yolo = arr_to_str(label_yolo, a1)\n","\n","#       # Save the string for txt file\n","#       txt_file.write(label_yolo)\n","#     txt_file.close()\n","    \n","\n","#   # Save images into folder \"images/train\"\n","#   for img in imgs:\n","#     b1 += 1\n","#     im = Image.fromarray(img, \"RGB\")\n","#     # im.save(f'./dataset_aug/images/train/rsbbsc_{b1}.jpg')\n","#     im.save(f'./dataset_origin/data_aug/randsafecrop/images/rsbbsc_{b1}.jpg')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tn38kJh-AdiI"},"source":["## Mosaic"]},{"cell_type":"code","metadata":{"id":"i2rzPJa7AdAB"},"source":["from sklearn.utils import shuffle\n","import random\n","\n","class FPTDatasetMosaic(Dataset):\n","  def __init__(self, dataframe, image_dir, transforms=None):\n","    super().__init__()\n","\n","    self.df = dataframe  # Annotation & Image's ID dataframe\n","    self.image_ids = shuffle(dataframe['image_id'].unique())  # Image's ID\n","    self.labels = [np.zeros((0, 4), dtype=np.float32)] * len(self.image_ids) # Image's bboxes\n","    # self.class_labels = [np.zeros((0, 1), dtype=np.float32)] * len(self.image_ids)  # Image's label\n","    self.img_size = 960\n","    self.image_dir = image_dir\n","    self.mosaic = True\n","    im_w = 1280\n","    im_h = 720\n","\n","    # Loop through each image (Each image might containt multiple bboxes & labels)\n","    for i, img_id in enumerate(self.image_ids):\n","      records = self.df[self.df['image_id'] == img_id]\n","      labels = records[['label', 'x', 'y', 'w', 'h']].values  # Annotations\n","      self.labels[i] = np.array(labels)\n","\n","      \n","  def __getitem__(self, index: int):\n","    if self.mosaic == True:\n","      # Load mosaic\n","      img, labels = load_mosaic(self, index)\n","      shapes = None\n","      img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # BGR to RGB      \n","      return img, labels\n","\n","  def __len__(self) -> int:\n","    return self.image_ids.shape[0]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ntuiUAqOINAg"},"source":["def random_affine(img, targets=(), degrees=10, translate=.1, scale=.1, shear=10, border=0):\n","    # torchvision.transforms.RandomAffine(degrees=(-10, 10), translate=(.1, .1), scale=(.9, 1.1), shear=(-10, 10))\n","    # https://medium.com/uruvideo/dataset-augmentation-with-random-homographies-a8f4b44830d4\n","\n","    if targets is None:  # targets = [cls, xyxy]\n","        targets = []\n","    height = img.shape[0] + border * 2\n","    width = img.shape[1] + border * 2\n","\n","    # Rotation and Scale\n","    R = np.eye(3)\n","    a = random.uniform(-degrees, degrees)\n","    # a += random.choice([-180, -90, 0, 90])  # add 90deg rotations to small rotations\n","    s = random.uniform(1 - scale, 1 + scale)\n","    R[:2] = cv2.getRotationMatrix2D(angle=a, center=(img.shape[1] / 2, img.shape[0] / 2), scale=s)\n","\n","    # Translation\n","    T = np.eye(3)\n","    T[0, 2] = random.uniform(-translate, translate) * img.shape[0] + border  # x translation (pixels)\n","    T[1, 2] = random.uniform(-translate, translate) * img.shape[1] + border  # y translation (pixels)\n","\n","    # Shear\n","    S = np.eye(3)\n","    S[0, 1] = math.tan(random.uniform(-shear, shear) * math.pi / 180)  # x shear (deg)\n","    S[1, 0] = math.tan(random.uniform(-shear, shear) * math.pi / 180)  # y shear (deg)\n","\n","    # Combined rotation matrix\n","    M = S @ T @ R  # ORDER IS IMPORTANT HERE!!\n","    if (border != 0) or (M != np.eye(3)).any():  # image changed\n","        img = cv2.warpAffine(img, M[:2], dsize=(width, height), flags=cv2.INTER_LINEAR, borderValue=(114, 114, 114))\n","\n","    # Transform label coordinates (Important!!!)\n","    n = len(targets)\n","    if n:\n","        # warp points\n","        xy = np.ones((n * 4, 3))\n","        xy[:, :2] = targets[:, [1, 2, 3, 4, 1, 4, 3, 2]].reshape(n * 4, 2)  # x1y1, x2y2, x1y2, x2y1\n","        xy = (xy @ M.T)[:, :2].reshape(n, 8)\n","\n","        # create new boxes\n","        x = xy[:, [0, 2, 4, 6]]\n","        y = xy[:, [1, 3, 5, 7]]\n","        xy = np.concatenate((x.min(1), y.min(1), x.max(1), y.max(1))).reshape(4, n).T\n","\n","        # # apply angle-based reduction of bounding boxes\n","        # radians = a * math.pi / 180\n","        # reduction = max(abs(math.sin(radians)), abs(math.cos(radians))) ** 0.5\n","        # x = (xy[:, 2] + xy[:, 0]) / 2\n","        # y = (xy[:, 3] + xy[:, 1]) / 2\n","        # w = (xy[:, 2] - xy[:, 0]) * reduction\n","        # h = (xy[:, 3] - xy[:, 1]) * reduction\n","        # xy = np.concatenate((x - w / 2, y - h / 2, x + w / 2, y + h / 2)).reshape(4, n).T\n","\n","        # Explanation of this part of code: https://github.com/ultralytics/yolov5/issues/448\n","        # reject warped points outside of image\n","        xy[:, [0, 2]] = xy[:, [0, 2]].clip(0, width)\n","        xy[:, [1, 3]] = xy[:, [1, 3]].clip(0, height)\n","        w = xy[:, 2] - xy[:, 0]\n","        h = xy[:, 3] - xy[:, 1]\n","        area = w * h\n","        area0 = (targets[:, 3] - targets[:, 1]) * (targets[:, 4] - targets[:, 2])\n","        ar = np.maximum(w / (h + 1e-16), h / (w + 1e-16))  # aspect ratio\n","        i = (w > 4) & (h > 4) & (area / (area0 * s + 1e-16) > 0.2) & (ar < 10)\n","\n","        targets = targets[i]\n","        targets[:, 1:5] = xy[i]\n","\n","    return img, targets"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F7am6SoPmbSg"},"source":["def load_mosaic(self, index):\n","    \"\"\"Load image in a mosaic form _ combines 4 training images into one in certain ratios (instead of only two in CutMix\"\"\"\n","\n","    labels4 = []\n","    s = self.img_size  # Size of image\n","    xc, yc = [int(random.uniform(s * 0.5, s * 1.5)) for _ in range(2)]  # mosaic center x, y\n","    indices = [index] + [random.randint(0, len(self.labels) - 1) for _ in range(3)]  # 3 additional image indices\n","    for i, index in enumerate(indices):\n","        # Load image\n","        img, (h, w) = load_image(self, index)\n","\n","        # place img in img4\n","        if i == 0:  # top left\n","            img4 = np.full(shape=(s * 2, s * 2, img.shape[2]), fill_value=114, dtype=np.uint8)  # base image with 4 tiles\n","            x1a, y1a, x2a, y2a = max(xc - w, 0), max(yc - h, 0), xc, yc  # xmin, ymin, xmax, ymax (large image)\n","            x1b, y1b, x2b, y2b = w - (x2a - x1a), h - (y2a - y1a), w, h  # xmin, ymin, xmax, ymax (small image)\n","        elif i == 1:  # top right\n","            x1a, y1a, x2a, y2a = xc, max(yc - h, 0), min(xc + w, s * 2), yc\n","            x1b, y1b, x2b, y2b = 0, h - (y2a - y1a), min(w, x2a - x1a), h\n","        elif i == 2:  # bottom left\n","            x1a, y1a, x2a, y2a = max(xc - w, 0), yc, xc, min(s * 2, yc + h)\n","            x1b, y1b, x2b, y2b = w - (x2a - x1a), 0, max(xc, w), min(y2a - y1a, h)\n","        elif i == 3:  # bottom right\n","            x1a, y1a, x2a, y2a = xc, yc, min(xc + w, s * 2), min(s * 2, yc + h)\n","            x1b, y1b, x2b, y2b = 0, 0, min(w, x2a - x1a), min(y2a - y1a, h)\n","\n","        img4[y1a:y2a, x1a:x2a] = img[y1b:y2b, x1b:x2b]  # img4[ymin:ymax, xmin:xmax]\n","        padw = x1a - x1b\n","        padh = y1a - y1b\n","\n","        # Labels\n","        x = self.labels[index]\n","        labels = x.copy()\n","        if x.size > 0:  # Normalized xywh to pixel xyxy format\n","            labels[:, 1] = w * (x[:, 1] - x[:, 3] / 2) + padw\n","            labels[:, 2] = h * (x[:, 2] - x[:, 4] / 2) + padh\n","            labels[:, 3] = w * (x[:, 1] + x[:, 3] / 2) + padw\n","            labels[:, 4] = h * (x[:, 2] + x[:, 4] / 2) + padh\n","        labels4.append(labels)\n","\n","    # Concat/clip labels\n","    if len(labels4):\n","        labels4 = np.concatenate(labels4, 0)\n","        # np.clip(labels4[:, 1:] - s / 2, 0, s, out=labels4[:, 1:])  # use with center crop\n","        np.clip(labels4[:, 1:], 0, 2 * s, out=labels4[:, 1:])  # use with random_affine\n","\n","    # Reason should add \"random_affine()\" in mosaic https://github.com/ultralytics/yolov5/issues/448\n","    img4, labels4 = random_affine(img4, labels4,\n","                                  degrees=1.98 * 2,\n","                                  translate=0.05 * 2,\n","                                  scale=0.05 * 2,\n","                                  shear=0.641 * 2,\n","                                  border=-s // 2)  # border to remove\n","\n","    return img4, labels4\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FttL3A0Wddm0"},"source":["def arr_to_str(arr, count_f):\n","  \"\"\"Transform arr to string & save them to folder dataset_aug\"\"\"\n","  result = str()\n","\n","  # Arr to String\n","  label_1 = int(arr[0])\n","  result = str(label_1) + ' '\n","  line = ' '.join([str(item) for item in arr[1:]]) + '\\n'\n","  result = result + line\n","  return result\n","\n","def pascal_to_yolo(xmin, ymin, xmax, ymax, image_width=640, image_height=640):\n","  x_coord = (xmin + xmax) / 2 / image_width\n","  y_coord = (ymin + ymax) / 2 / image_height\n","  shape_width = (xmax - xmin) / image_width\n","  shape_height = (ymax - ymin) / image_height\n","  return x_coord, y_coord, shape_width, shape_height\n","\n","def collate_fn(batch):\n","    return tuple(zip(*batch))\n","\n","import random, math\n","import tensorflow as tf\n","\n","def load_image(self, index):\n","  # loads 1 image from dataset, returns img, original hw, resized hw\n","  \"\"\"Load 1 image from dataset\n","  Input:\n","    index: idx to search for image's id\n","  Output:\n","    img, hw_original, hw_resized \"\"\"\n","  # Read an image using opencv2\n","  image_id = self.image_ids[index]\n","  img = cv2.imread(f'{self.image_dir}/{image_id}.jpg', cv2.IMREAD_COLOR)\n","    \n","  assert img is not None, 'Image Not Found ' + imgpath\n","  h0, w0 = img.shape[:2]  # orig hw\n","  return img, (h0, w0)  # img, hw_original"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-h7jZ6RSfYrx"},"source":["train_dataset = FPTDatasetMosaic(train_df, DIR_TRAIN)  # 792 images\n","train_data_loader = DataLoader(\n","    train_dataset,\n","    batch_size=15,\n","    shuffle=True,\n","    num_workers=4,\n","    collate_fn=collate_fn\n",")\n","\n","def mosaic_filter(num_img):\n","  \"\"\"\n","    Filter only mosaic image with number of labels larger than zero\n","    Input:\n","      num_img: # of wanted mosaic images\n","    Output:\n","      image_lst: list of mosaic images\n","      target_lst: list of appropriate mosaic labels\n","  \"\"\"\n","  a = 0\n","  image_lst = list()\n","  target_lst = list()\n","\n","  while (a < num_img):\n","    images, targets = next(iter(train_data_loader))\n","    for image, target in zip(images, targets):\n","      if len(target) > 1:\n","        image_lst.append(image)\n","        target_lst.append(target)\n","        a += 1\n","      else:\n","        continue\n","\n","      if a == num_img:\n","        break\n","\n","  return image_lst, target_lst"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zZMUckuTixzH"},"source":["# Construct 200 mosaic images\n","images, targets = mosaic_filter(144)\n","\n","# Plot some filtered mosaic images\n","fig, ax = plt.subplots(6, 6, figsize=(40, 40))\n","ax = ax.flatten()\n","\n","for i in range(36):  # Go through each batch \n","    boxes = targets[i]\n","    sample = images[i]\n","\n","    for box in boxes:\n","        cv2.rectangle(sample,\n","                  (int(box[1]), int(box[2])),\n","                  (int(box[3]), int(box[4])),\n","                  220, 3)\n","\n","    ax[i].imshow(sample)  # Visualize a sample for each batch\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# UNCOMENT TO SAVE CREATED MOSAIC .JPG IMAGES & .TXT LABELS INTO FOLDER\n","# a1 = 0\n","# b1 = 0\n","\n","# for img, label in zip(images, targets):\n","#   height, width = img.shape[:2]\n","#   a1 += 1\n","\n","#   # txt_file = open(f'./dataset_aug/labels/train/img_mosaic_{a1}.txt', 'w')\n","#   txt_file = open(f'./dataset_origin/data_aug/mosaic/labels/img_mosaic_{a1}_a.txt', 'w')\n","#   # Through each bbox of an image\n","#   for j in range(len(label)):\n","#     # Normalize the box's annotation after augmentation (AS requirement from competition)\n","#     a,b,c,d = pascal_to_yolo(label[j][1], label[j][2], label[j][3], label[j][4], width, height)\n","#     label_yolo = np.array([label[j][0], a, b, c, d])\n","#     label_yolo = arr_to_str(label_yolo, a1)\n","\n","#     # Save the string for txt file\n","#     txt_file.write(label_yolo)\n","#   txt_file.close()\n","    \n","\n","# # Save images into folder \"images/train\"\n","# for img in images:\n","#   b1 += 1\n","#   im = Image.fromarray(img, \"RGB\")\n","#   # im.save(f'./dataset_aug/images/train/img_mosaic_b{b1}.jpg')\n","#   im.save(f'./dataset_origin/data_aug/mosaic/images/img_mosaic_{b1}_a.jpg')"],"metadata":{"id":"9WURajxgkl8Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8kE20dPbwrLy"},"source":[""],"execution_count":null,"outputs":[]}]}